#!/usr/bin/env python3
"""
Ultimate n8n Node Documentation Extractor - Maximum Detail for AI Training
Extracts every possible detail from n8n nodes for comprehensive LLM training data

Features:
- Complete TypeScript parsing with import resolution
- Real-world workflow examples extraction
- Credential requirements and authentication flows
- Expression pattern detection and analysis
- Validation rules and constraints
- Type information and schemas
- UI hints, tooltips, and placeholders
- Conditional field visibility (displayOptions)
- Multi-resource node support
- Webhook and trigger configurations
- API endpoint patterns
- Error messages and edge cases
"""

import sys
import re
import json
import yaml
import copy
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import date
from collections import defaultdict

# Configuration
CURRENT_DIR = Path("/media/tyler/fastraid/Projects/n8n Node Scrapper")
N8N_REPO = CURRENT_DIR / "n8n"
OUTPUT_DIR = CURRENT_DIR / "extracted_docs"
TEMPLATE_PATH = Path("/media/tyler/fastraid/Games/n8n_Node_Default_Settings_Template.md")

# Standard n8n node execution settings (from n8n frontend/workflow interfaces)
# These are framework-level settings that apply to nodes universally

# Settings for ALL nodes (both executable and trigger nodes)
COMMON_SETTINGS = {
    "notes": {
        "name": "Notes",
        "field_id": "notes",
        "type": "string",
        "default": "",
        "description": "Optional note to save with the node"
    },
    "notesInFlow": {
        "name": "Display Note in Flow?",
        "field_id": "notesInFlow",
        "type": "boolean",
        "default": False,
        "description": "If active, the note above will display in the flow as a subtitle"
    }
}

# Additional settings for EXECUTABLE nodes only (not trigger nodes)
EXECUTABLE_SETTINGS = {
    "alwaysOutputData": {
        "name": "Always Output Data",
        "field_id": "alwaysOutputData",
        "type": "boolean",
        "default": False,
        "description": "If active, will output a single, empty item when the output would have been empty. Use to prevent the workflow finishing on this node"
    },
    "executeOnce": {
        "name": "Execute Once",
        "field_id": "executeOnce",
        "type": "boolean",
        "default": False,
        "description": "If active, the node executes only once, with data from the first item it receives"
    },
    "retryOnFail": {
        "name": "Retry On Fail",
        "field_id": "retryOnFail",
        "type": "boolean",
        "default": False,
        "description": "If active, the node tries to execute again when it fails"
    },
    "maxTries": {
        "name": "Max. Tries",
        "field_id": "maxTries",
        "type": "number",
        "default": 3,
        "min": 2,
        "max": 5,
        "description": "Number of times to attempt to execute the node before failing the execution",
        "displayOptions": {
            "show": {
                "retryOnFail": [True]
            }
        }
    },
    "waitBetweenTries": {
        "name": "Wait Between Tries (ms)",
        "field_id": "waitBetweenTries",
        "type": "number",
        "default": 1000,
        "min": 0,
        "max": 5000,
        "description": "How long to wait between each attempt (in milliseconds)",
        "displayOptions": {
            "show": {
                "retryOnFail": [True]
            }
        }
    },
    "onError": {
        "name": "On Error",
        "field_id": "onError",
        "type": "options",
        "default": "stopWorkflow",
        "description": "Action to take when the node execution fails",
        "options": [
            {
                "name": "Stop Workflow",
                "value": "stopWorkflow",
                "description": "Halt execution and fail workflow"
            },
            {
                "name": "Continue",
                "value": "continueRegularOutput",
                "description": "Pass error message as item in regular output"
            },
            {
                "name": "Continue (using error output)",
                "value": "continueErrorOutput",
                "description": "Pass item to an extra `error` output"
            }
        ]
    }
}


class WorkflowParser:
    """Parse workflow.json files to extract real-world examples"""

    @staticmethod
    def find_workflow_files(node_dir: Path) -> List[Path]:
        """Find all workflow.json files for a node"""
        workflow_files = []

        # Check test directory
        test_dir = node_dir / "test"
        if test_dir.exists():
            workflow_files.extend(test_dir.glob("**/*.workflow.json"))

        # Check __tests__ directory
        tests_dir = node_dir / "__tests__"
        if tests_dir.exists():
            workflow_files.extend(tests_dir.glob("**/*.workflow.json"))

        # Check __test__ directory
        test_dir2 = node_dir / "__test__"
        if test_dir2.exists():
            workflow_files.extend(test_dir2.glob("**/*.workflow.json"))

        return list(workflow_files)

    @staticmethod
    def parse_workflow(workflow_path: Path, node_type: str) -> List[Dict[str, Any]]:
        """Extract example configurations for a specific node from workflow"""
        try:
            with open(workflow_path, 'r', encoding='utf-8') as f:
                workflow = json.load(f)

            examples = []
            for node in workflow.get('nodes', []):
                if node_type in node.get('type', ''):
                    example = {
                        'name': node.get('name', 'Unnamed'),
                        'parameters': node.get('parameters', {}),
                        'credentials': node.get('credentials', {}),
                        'position': node.get('position', {}),
                        'workflow_file': workflow_path.name,
                        'workflow_name': workflow.get('name', 'Unnamed workflow')
                    }
                    examples.append(example)

            return examples
        except Exception as e:
            return []


class CredentialExtractor:
    """Extract credential requirements from node definitions"""

    @staticmethod
    def extract_credentials(content: str) -> List[Dict[str, Any]]:
        """Extract credential definitions from TypeScript"""
        credentials = []

        array_start_match = re.search(r'credentials:\s*\[', content)
        if not array_start_match:
            return credentials

        start_index = array_start_match.end()
        depth = 1
        i = start_index

        while i < len(content) and depth > 0:
            char = content[i]
            if char == '[':
                depth += 1
            elif char == ']':
                depth -= 1
            i += 1

        creds_str = content[start_index:i-1] if depth == 0 else ''

        if not creds_str:
            return credentials

        objects, _ = TypeScriptParser._split_into_objects(creds_str)

        for obj_str in objects:
            cred: Dict[str, Any] = {}

            name_match = re.search(r"name:\s*['\"]([^'\"]*)['\"]", obj_str)
            if name_match:
                cred['name'] = name_match.group(1)

            cred['required'] = bool(re.search(r"required:\s*true", obj_str))

            display_match = re.search(r"displayOptions:\s*\{", obj_str)
            if display_match:
                display_start = display_match.end()
                display_depth = 1
                j = display_start
                while j < len(obj_str) and display_depth > 0:
                    if obj_str[j] == '{':
                        display_depth += 1
                    elif obj_str[j] == '}':
                        display_depth -= 1
                    j += 1
                if display_depth == 0:
                    display_content = obj_str[display_start:j-1]
                    cred['displayOptions'] = TypeScriptParser._parse_display_options(display_content)

            tested_by_match = re.search(r"testedBy:\s*\{([\s\S]*?)\}", obj_str)
            if tested_by_match:
                cred['testedBy'] = tested_by_match.group(1).strip()

            # Only add credential if it has meaningful data (name is required)
            if cred.get('name'):
                # Remove 'required' field if it's the only field besides 'name'
                # to avoid bloating output with required: false for all credentials
                credentials.append(cred)

        return credentials


class ExpressionExtractor:
    """Extract expression examples from code and tests"""

    @staticmethod
    def find_expressions(content: str) -> List[str]:
        """Find n8n expression patterns in code"""
        expressions = set()

        # Common expression patterns
        patterns = [
            r'\{\{\s*\$json\.[^}]+\}\}',                      # {{ $json.field }}
            r'\{\{\s*\$node\[.*?\]\.json\.[^}]+\}\}',         # {{ $node["Name"].json.field }}
            r'\{\{\s*\$now\.[^}]+\}\}',                       # {{ $now.format(...) }}
            r'\{\{\s*\$input\.[^}]+\}\}',                     # {{ $input.item.json }}
            r'\{\{\s*\$item\([^)]+\)[^}]+\}\}',               # {{ $item(0).json }}
            r'\{\{\s*\$parameter\.[^}]+\}\}',                 # {{ $parameter.field }}
            r'\{\{\s*\$workflow\.[^}]+\}\}',                  # {{ $workflow.id }}
            r'\{\{\s*\$execution\.[^}]+\}\}',                 # {{ $execution.id }}
            r'\{\{\s*\$today\.[^}]+\}\}',                     # {{ $today.format(...) }}
            r'\{\{\s*\$binary\.[^}]+\}\}',                    # {{ $binary.data }}
            r'=\s*\{\{.*?\}\}',                                # expression assignments
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content)
            # Filter out meta-expressions (JavaScript template literals within expressions)
            # These are code that generates expressions, not actual user expressions
            for match in matches:
                if '${' not in match:  # Exclude expressions containing JavaScript template literals
                    expressions.add(match)

        return list(expressions)


class ValidationExtractor:
    """Extract validation rules from properties"""

    @staticmethod
    def extract_validation_rules(prop: Dict[str, Any]) -> Dict[str, Any]:
        """Extract validation constraints from a property"""
        rules = {}

        if prop.get('required'):
            rules['required'] = True

        if prop.get('type') == 'string':
            if 'validate' in str(prop):
                rules['format'] = 'validated'

        # URL validation
        if 'url' in prop.get('name', '').lower():
            rules['format'] = 'url'

        # Email validation
        if 'email' in prop.get('name', '').lower():
            rules['format'] = 'email'

        # Extract display options (conditional display)
        if prop.get('displayOptions'):
            rules['displayOptions'] = prop['displayOptions']

        # Extract min/max if present
        if 'min' in prop:
            rules['min'] = prop['min']
        if 'max' in prop:
            rules['max'] = prop['max']

        return rules


class TypeInfoExtractor:
    """Extract detailed type information"""

    @staticmethod
    def extract_type_info(prop: Dict[str, Any]) -> Dict[str, Any]:
        """Extract comprehensive type information"""
        type_info = {
            'type': prop.get('type', 'unknown'),
            'displayName': prop.get('displayName', ''),
            'name': prop.get('name', '')
        }

        # Extract typeOptions if present
        if prop.get('typeOptions'):
            type_info['typeOptions'] = prop['typeOptions']

        # For options/multiOptions, include all possible values
        if prop.get('type') in ['options', 'multiOptions']:
            type_info['possibleValues'] = []
            for opt in prop.get('options', []):
                type_info['possibleValues'].append({
                    'value': opt.get('value', opt.get('name', '')),
                    'name': opt.get('name', ''),
                    'description': opt.get('description', '')
                })

        # For collections, include sub-properties
        if prop.get('type') in ['collection', 'fixedCollection']:
            type_info['subProperties'] = prop.get('options', [])

        return type_info


class APIPatternExtractor:
    """Extract API endpoint and HTTP patterns"""

    @staticmethod
    def find_api_patterns(content: str) -> Dict[str, Any]:
        """Find API patterns in node code"""
        patterns = {
            'http_methods': set(),
            'endpoints': [],
            'headers': set(),
            'query_params': set()
        }

        # Find HTTP methods
        methods = re.findall(r'method:\s*[\'"]([A-Z]+)[\'"]', content)
        patterns['http_methods'].update(methods)

        # Find URL patterns
        urls = re.findall(r'url:\s*[\'"`]([^\'"`]+)[\'"`]', content)
        # Clean template variables from URLs: ${var} -> {var}
        clean_urls = []
        for url in urls:
            # Replace ${variable} with {variable} to show it's a parameter
            clean_url = re.sub(r'\$\{([^}]+)\}', r'{\1}', url)
            clean_urls.append(clean_url)
        patterns['endpoints'].extend(clean_urls)

        # Find headers
        headers = re.findall(r'headers:\s*\{([^}]+)\}', content)
        for header_block in headers:
            header_names = re.findall(r'[\'"]([A-Za-z-]+)[\'"]', header_block)
            patterns['headers'].update(header_names)

        # Find query parameters
        query_params = re.findall(r'qs:\s*\{([^}]+)\}', content)
        for param_block in query_params:
            param_names = re.findall(r'[\'"]([A-Za-z_]+)[\'"]', param_block)
            patterns['query_params'].update(param_names)

        # Convert sets to lists for JSON serialization
        patterns['http_methods'] = list(patterns['http_methods'])
        patterns['headers'] = list(patterns['headers'])
        patterns['query_params'] = list(patterns['query_params'])

        return patterns


class TypeScriptParser:
    """Parse TypeScript code to extract node definitions"""

    @staticmethod
    def parse_imports(content: str, node_path: Path) -> Dict[str, List[str]]:
        """
        Parse import statements to find description files
        Returns: Dict mapping description file paths to imported variable names
        Example: {'/path/to/AccountDescription.ts': ['accountFields', 'accountOperations'],
                  '/path/to/Database.resource.ts': ['database']}
        """
        imports = {}
        node_dir = node_path.parent

        # Pattern 1: import { var1, var2 } from './FileName' or '../FileName';
        named_import_pattern = r"import\s+\{([^}]+)\}\s+from\s+['\"]([^'\"]+)['\"]"
        for match in re.finditer(named_import_pattern, content):
            imported_names = match.group(1)
            source_file = match.group(2)

            # Process relative imports (both ./ and ../)
            if source_file.startswith('./') or source_file.startswith('../'):
                # Clean up imported names
                names = [name.strip() for name in imported_names.split(',')]

                # Resolve the file path relative to node_dir
                # For './', remove the prefix. For '../', resolve parent directory
                if source_file.startswith('./'):
                    relative_path = source_file.replace('./', '')
                    base_dir = node_dir
                else:
                    # Handle '../' imports
                    relative_path = source_file
                    base_dir = node_dir

                # Resolve the path using Path.resolve (handles ../ properly)
                try:
                    # Try direct .ts file first
                    desc_file_path = (base_dir / relative_path).resolve()
                    if not str(desc_file_path).endswith('.ts'):
                        desc_file_path = Path(str(desc_file_path) + '.ts')

                    # If not found, try as directory with index.ts (common pattern for './descriptions')
                    if not desc_file_path.exists():
                        # Try without .ts extension (might be a directory)
                        dir_path = (base_dir / relative_path).resolve()
                        index_path = dir_path / "index.ts"
                        if index_path.exists():
                            # This is a re-export index file - parse it to find actual files
                            try:
                                with open(index_path, 'r', encoding='utf-8') as f:
                                    index_content = f.read()

                                # Find re-export statements: export * from './SomeFile'
                                reexport_pattern = r"export\s+\*\s+from\s+['\"]\.\/([^'\"]+)['\"]"
                                for reexport_match in re.finditer(reexport_pattern, index_content):
                                    reexport_file = reexport_match.group(1)
                                    # Resolve re-export path relative to index.ts directory
                                    reexport_path = index_path.parent / f"{reexport_file}.ts"
                                    if reexport_path.exists():
                                        # Add all imported names to this re-exported file
                                        imports[str(reexport_path)] = names
                            except Exception:
                                # If we can't parse index.ts, continue
                                pass
                        else:
                            # Neither .ts file nor index.ts found - skip this import
                            continue
                    else:
                        # Direct .ts file found
                        imports[str(desc_file_path)] = names
                except Exception:
                    # If path resolution fails, skip this import
                    continue

        # Pattern 2: import * as name from './FileName' or '../FileName';
        namespace_import_pattern = r"import\s+\*\s+as\s+(\w+)\s+from\s+['\"]([^'\"]+)['\"]"
        for match in re.finditer(namespace_import_pattern, content):
            namespace_name = match.group(1)
            source_file = match.group(2)

            # Process relative imports (both ./ and ../)
            if source_file.startswith('./') or source_file.startswith('../'):
                try:
                    # Resolve the file path relative to node_dir (handles ../ properly)
                    desc_file_path = (node_dir / source_file).resolve()
                    if not str(desc_file_path).endswith('.ts'):
                        desc_file_path = Path(str(desc_file_path) + '.ts')

                    if desc_file_path.exists():
                        # For namespace imports, the imported name is the namespace itself
                        imports[str(desc_file_path)] = [namespace_name]
                except Exception:
                    # If path resolution fails, skip this import
                    continue

        return imports

    @staticmethod
    def find_version_description_file(node_path: Path) -> Optional[Path]:
        """
        Find versionDescription.ts file for versioned nodes.

        Patterns to check:
        - ./versionDescription.ts or ./VersionDescription.ts (same directory)
        - ./v1/versionDescription.ts, ./v2/versionDescription.ts
        - ../versionDescription.ts (parent directory)
        - ./actions/versionDescription.ts (Postgres pattern)
        """
        node_dir = node_path.parent

        # Try both capitalization variants
        filename_variants = ['versionDescription.ts', 'VersionDescription.ts']

        # Pattern 1: Same directory
        for variant in filename_variants:
            candidate = node_dir / variant
            if candidate.exists():
                return candidate

        # Pattern 2: actions subdirectory (PostgresV2 pattern)
        for variant in filename_variants:
            candidate = node_dir / 'actions' / variant
            if candidate.exists():
                return candidate

        # Pattern 3: Parent directory (if we're in v1/, v2/, etc.)
        if node_dir.name in ['v1', 'v2', 'v3', 'V1', 'V2', 'V3']:
            for variant in filename_variants:
                candidate = node_dir.parent / variant
                if candidate.exists():
                    return candidate

        return None

    @staticmethod
    def extract_exported_property(content: str, property_name: str) -> Optional[Dict[str, Any]]:
        """
        Extract a single exported property object from TypeScript code
        Example: export const eventDisplay: INodeProperties = {...}
        Args:
            content: TypeScript file content
            property_name: Name of the exported property to extract
        Returns:
            Parsed property object or None if not found
        """
        # Pattern to find the exported property object (with or without type annotation)
        patterns = [
            rf"export\s+const\s+{property_name}\s*:\s*INodeProperties\s*=\s*\{{",  # With type
            rf"export\s+const\s+{property_name}\s*=\s*\{{",  # Without type
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                # Find the full object using brace counting
                start_pos = match.end() - 1  # Position of the opening {
                depth = 1
                i = start_pos + 1

                while i < len(content) and depth > 0:
                    if content[i] == '{':
                        depth += 1
                    elif content[i] == '}':
                        depth -= 1
                    i += 1

                if depth == 0:
                    # Extract and parse the object
                    obj_content = content[start_pos:i]
                    prop = TypeScriptParser._parse_single_property(obj_content)
                    return prop

        return None

    @staticmethod
    def extract_exported_array(content: str, array_name: str, source_path: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        Extract an exported array from TypeScript code, resolving spreads
        Example: export const accountOperations: INodeProperties[] = [...]
        Args:
            content: TypeScript file content
            array_name: Name of the exported array to extract
            source_path: Path to the file being parsed (for resolving spreads in nested arrays)
        """
        # Pattern to find the exported array
        pattern = rf"export\s+const\s+{array_name}\s*:\s*INodeProperties\[\]\s*=\s*\["
        match = re.search(pattern, content)

        if not match:
            return []

        # Find the full array using bracket counting
        start_pos = match.end() - 1  # Position of the opening [
        depth = 1
        i = start_pos + 1

        while i < len(content) and depth > 0:
            if content[i] == '[':
                depth += 1
            elif content[i] == ']':
                depth -= 1
            i += 1

        if depth == 0:
            array_content = content[start_pos+1:i-1]  # Extract content between [ and ]
            # Parse the array elements, including spreads
            props, spreads = TypeScriptParser._parse_property_objects(array_content)

            # Resolve spread operators if source_path provided
            if spreads and source_path:
                # Parse imports from this file to resolve spreads
                imports = TypeScriptParser.parse_imports(content, source_path)
                resolved_props = TypeScriptParser._resolve_spreads(spreads, imports, source_path, content)
                props.extend(resolved_props)

            return props

        return []

    @staticmethod
    def extract_description_object(content: str, const_name: str) -> Dict[str, Any]:
        """Extract properties, credentials, and metadata from a description object export."""
        pattern = rf"export\s+const\s+{const_name}[^=]*=\s*\{{"
        match = re.search(pattern, content)
        if not match:
            return {}

        start_pos = match.end() - 1
        depth = 1
        i = start_pos + 1

        while i < len(content) and depth > 0:
            if content[i] == '{':
                depth += 1
            elif content[i] == '}':
                depth -= 1
            i += 1

        if depth != 0:
            return {}

        object_body = content[start_pos + 1: i - 1]
        result: Dict[str, Any] = {}

        # === METADATA FIELDS ===

        # Extract inputs
        inputs_match = re.search(r"inputs:\s*\[([^\]]+)\]", object_body)
        if inputs_match:
            inputs_str = inputs_match.group(1)
            input_types = re.findall(r"NodeConnectionTypes\.(\w+)", inputs_str)
            if input_types:
                result['inputs'] = input_types

        # Extract outputs
        outputs_match = re.search(r"outputs:\s*\[([^\]]+)\]", object_body)
        if outputs_match:
            outputs_str = outputs_match.group(1)
            output_types = re.findall(r"NodeConnectionTypes\.(\w+)", outputs_str)
            if output_types:
                result['outputs'] = output_types

        # Extract group
        group_match = re.search(r"group:\s*\[([^\]]+)\]", object_body)
        if group_match:
            group_str = group_match.group(1)
            groups = re.findall(r"""['"](.*?)['"]""", group_str)
            if groups:
                result['group'] = groups

        # Extract icon (string format)
        icon_match = re.search(r"""icon:\s*['"](.*?)['"]""", object_body)
        if icon_match:
            result['icon'] = icon_match.group(1)

        # Extract displayName
        display_name = TypeScriptParser.extract_string_value(object_body, 'displayName')
        if display_name:
            result['displayName'] = display_name

        # Extract subtitle
        subtitle = TypeScriptParser.extract_string_value(object_body, 'subtitle')
        if subtitle:
            result['subtitle'] = subtitle

        # Extract description
        desc = TypeScriptParser.extract_string_value(object_body, 'description')
        if desc:
            result['description'] = desc

        # Extract usableAsTool
        usable_as_tool_match = re.search(r"usableAsTool:\s*(true|false)", object_body)
        if usable_as_tool_match:
            result['usableAsTool'] = usable_as_tool_match.group(1) == 'true'

        # === PROPERTIES ===
        props_match = re.search(r"properties:\s*\[", object_body)
        if props_match:
            props_start = props_match.end() - 1
            depth = 1
            j = props_start + 1
            while j < len(object_body) and depth > 0:
                if object_body[j] == '[':
                    depth += 1
                elif object_body[j] == ']':
                    depth -= 1
                j += 1
            if depth == 0:
                props_content = object_body[props_start + 1: j - 1]
                # Parse properties (ignore spreads at this level)
                props, spreads = TypeScriptParser._parse_property_objects(props_content)
                result['properties'] = props

        # === CREDENTIALS ===
        cred_match = re.search(r"credentials:\s*\[", object_body)
        if cred_match:
            cred_start = cred_match.end()
            depth = 1
            k = cred_start
            while k < len(object_body) and depth > 0:
                if object_body[k] == '[':
                    depth += 1
                elif object_body[k] == ']':
                    depth -= 1
                k += 1
            if depth == 0:
                cred_content = object_body[cred_start: k - 1]
                credentials = []
                for cred_str in TypeScriptParser._split_into_objects(cred_content):
                    cred: Dict[str, Any] = {}
                    name = TypeScriptParser.extract_string_value(cred_str, 'name')
                    if not name:
                        continue
                    cred['name'] = name
                    cred['required'] = bool(re.search(r"required:\s*true", cred_str))

                    display_match = re.search(r"displayOptions:\s*\{", cred_str)
                    if display_match:
                        display_start = display_match.end()
                        display_depth = 1
                        m = display_start
                        while m < len(cred_str) and display_depth > 0:
                            if cred_str[m] == '{':
                                display_depth += 1
                            elif cred_str[m] == '}':
                                display_depth -= 1
                            m += 1
                        if display_depth == 0:
                            display_content = cred_str[display_start: m - 1]
                            cred['displayOptions'] = TypeScriptParser._parse_display_options(display_content)
                    credentials.append(cred)

                result['credentials'] = credentials

        return result

    @staticmethod
    def extract_array_value(content: str, field: str) -> List[str]:
        """Extract an array field value"""
        array_match = re.search(rf"{field}:\s*\[([\s\S]*?)\]", content)
        if not array_match:
            return []

        array_content = array_match.group(1)
        # Extract string values from array
        values = re.findall(r"['\"]([^'\"]+)['\"]", array_content)
        return values

    @staticmethod
    def extract_object_value(content: str, field: str) -> Optional[Dict[str, Any]]:
        """Extract an object field value"""
        obj_match = re.search(rf"{field}:\s*\{{", content)
        if not obj_match:
            return None

        start_pos = obj_match.end() - 1
        depth = 1
        i = start_pos + 1
        while i < len(content) and depth > 0:
            if content[i] == '{':
                depth += 1
            elif content[i] == '}':
                depth -= 1
            i += 1

        if depth == 0:
            obj_content = content[start_pos+1:i-1]
            # Try to parse as JSON-like
            result = {}
            # Extract key-value pairs
            for match in re.finditer(r"(\w+):\s*(['\"]([^'\"]+)['\"]|(true|false)|(\d+)|(\{[^\}]+\}))", obj_content):
                key = match.group(1)
                if match.group(3):  # String value
                    result[key] = match.group(3)
                elif match.group(4):  # Boolean
                    result[key] = match.group(4) == 'true'
                elif match.group(5):  # Number
                    result[key] = int(match.group(5))
                elif match.group(6):  # Nested object
                    result[key] = match.group(6)
            return result if result else None
        return None

    @staticmethod
    def resolve_template_variables(text: str, content: str) -> str:
        """Resolve ${VARIABLE} template references by finding const declarations"""
        if not text or '${' not in text:
            return text

        # Find all ${...} patterns (including complex expressions)
        template_pattern = r'\$\{([^}]+)\}'
        matches = re.finditer(template_pattern, text)

        for match in matches:
            full_expr = match.group(0)  # ${...}
            inner_expr = match.group(1)  # Content inside ${}

            # Check if it's a simple variable (only word characters) or complex expression
            is_simple = re.match(r'^\w+$', inner_expr)

            if is_simple:
                # Try to resolve simple variable
                var_name = inner_expr
                patterns = [
                    # const VAR = 'value' or const VAR = "value" or const VAR = `value`
                    rf"const\s+{var_name}\s*=\s*['\"`]([^'\"` ]+)['\"`]",
                    # const VAR = 'multi-line value'
                    rf"const\s+{var_name}\s*=\s*['\"]([^'\"]*)['\"]",
                ]

                resolved = False
                for pattern in patterns:
                    var_match = re.search(pattern, content, re.DOTALL)
                    if var_match:
                        value = var_match.group(1).strip()
                        text = text.replace(full_expr, value)
                        resolved = True
                        break

                # If not resolved, try to find it as an exported const
                if not resolved:
                    export_pattern = rf"export\s+const\s+{var_name}\s*=\s*['\"`]([^'\"` ]+)['\"`]"
                    export_match = re.search(export_pattern, content)
                    if export_match:
                        value = export_match.group(1).strip()
                        text = text.replace(full_expr, value)
                        resolved = True

                # If still not resolved, mark as unresolvable
                if not resolved:
                    text = text.replace(full_expr, f'(template: {inner_expr})')
            else:
                # Complex expression (has ., (, [, etc.) - mark as unresolvable template
                text = text.replace(full_expr, f'(template: {inner_expr})')

        return text

    @staticmethod
    def extract_string_value(content: str, field: str) -> Optional[str]:
        """Extract a string field value, handling nested quotes and multi-line strings"""
        # Safety check: ensure content is a string
        if not isinstance(content, str):
            return None

        # Look for the field
        field_match = re.search(rf"{field}:\s*(['\"`])", content)
        if not field_match:
            return None

        quote_char = field_match.group(1)
        start_pos = field_match.end()
        result_str = None

        # For template literals, handle differently
        if quote_char == '`':
            # Match until closing backtick, handling escaped backticks
            pattern = rf"{field}:\s*`((?:[^`\\]|\\.)*)`"
            match = re.search(pattern, content, re.DOTALL)
            if match:
                result_str = match.group(1).strip()
        else:
            # For regular quotes, manually parse to handle nested quotes
            result = []
            i = start_pos
            escaped = False

            while i < len(content):
                char = content[i]

                if escaped:
                    result.append(char)
                    escaped = False
                elif char == '\\':
                    escaped = True
                elif char == quote_char:
                    # Found closing quote
                    result_str = ''.join(result).strip()
                    break
                else:
                    result.append(char)

                i += 1

        # Resolve template variables if present
        if result_str:
            result_str = TypeScriptParser.resolve_template_variables(result_str, content)

        return result_str

    @staticmethod
    def extract_properties_array(content: str, node_path: Optional[Path] = None) -> List[Dict[str, Any]]:
        """Extract and parse the properties array, resolving spread operators"""
        patterns = [
            r'export\s+const\s+\w+:\s*INodeProperties\[\]\s*=\s*\[',
            r'export\s+const\s+properties:\s*INodeProperties\[\]\s*=\s*\[',
            r'const\s+\w+:\s*INodeProperties\[\]\s*=\s*\[',
            r'properties:\s*\[',
        ]

        props_str = ""
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                start_pos = match.end()
                depth = 1
                i = start_pos

                while i < len(content) and depth > 0:
                    char = content[i]
                    if char == '[':
                        depth += 1
                    elif char == ']':
                        depth -= 1
                    i += 1

                if depth == 0:
                    props_str = content[start_pos:i-1]
                    break

        if not props_str:
            return []

        properties, spreads = TypeScriptParser._parse_property_objects(props_str)

        # Resolve spread operators if node_path is provided
        if spreads and node_path:
            imports = TypeScriptParser.parse_imports(content, node_path)
            resolved_props = TypeScriptParser._resolve_spreads(spreads, imports, node_path, content)
            properties.extend(resolved_props)

        return properties

    @staticmethod
    def _parse_property_objects(props_str: str) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        Parse property objects from properties array string
        Returns: (properties, spread_variables) tuple
        """
        properties = []
        objects, spreads = TypeScriptParser._split_into_objects(props_str)

        for obj_str in objects:
            prop = TypeScriptParser._parse_single_property(obj_str)
            if prop:
                properties.append(prop)

        return properties, spreads

    @staticmethod
    def _split_into_objects(text: str) -> Tuple[List[str], List[str]]:
        """
        Split a string into individual object strings and spread operators
        Returns: (objects, spreads) where spreads are variable names or property access like ['arrayName1', 'module.property']
        """
        objects = []
        spreads = []
        depth = 0
        current = ""
        i = 0

        while i < len(text):
            char = text[i]

            # Check for spread operator (handles both ...variable and ...module.property)
            if char == '.' and i + 2 < len(text) and text[i+1:i+3] == '..':
                # Updated regex to capture property access: ...module.property or ...module.property.subproperty
                spread_match = re.match(r'\.\.\.(\w+(?:\.\w+)*)', text[i:])
                if spread_match:
                    spread_var = spread_match.group(1)  # Captures "module.property" or just "variable"
                    spreads.append(spread_var)  # Collect spread variable/property name
                    i += len(spread_match.group(0))
                    while i < len(text) and text[i] in [',', ' ', '\n', '\t']:
                        i += 1
                    continue

            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
            elif char == ',' and depth == 0:
                # Comma at depth 0 - end of current item
                cleaned = current.strip()
                if cleaned and not cleaned.startswith('...'):
                    # Check if this is a bare identifier reference (e.g., "eventDisplay")
                    # Pattern: just a word character sequence, no braces
                    if re.match(r'^\w+$', cleaned):
                        # Bare identifier - treat as spread for resolution
                        spreads.append(cleaned)
                    else:
                        # Object literal
                        objects.append(cleaned)
                current = ""
                i += 1
                continue

            current += char
            i += 1

        # Handle any remaining content
        if current.strip() and depth == 0:
            cleaned = current.strip().rstrip(',').strip()
            if cleaned and cleaned != ',' and not cleaned.startswith('...'):
                # Check if this is a bare identifier reference
                if re.match(r'^\w+$', cleaned):
                    spreads.append(cleaned)
                else:
                    objects.append(cleaned)

        return objects, spreads

    @staticmethod
    def _resolve_spreads(spread_vars: List[str], imports: Dict[str, List[str]], node_path: Path, current_content: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Resolve spread operators by loading and parsing the imported or local arrays
        Args:
            spread_vars: List of variable names from spread operators (e.g., ['javascriptCodeDescription', 'database.description'])
            imports: Dict mapping file paths to imported variable names
            node_path: Path to the current node file (for resolving relative imports)
            current_content: Content of the current file (to check for local const arrays)
        Returns:
            List of resolved property objects
        """
        resolved_properties = []

        # Create reverse lookup: variable name -> file path
        var_to_file = {}
        for file_path, var_names in imports.items():
            for var_name in var_names:
                var_to_file[var_name] = file_path

        for spread_var in spread_vars:
            # Check if this is a property access spread (e.g., "database.description")
            if '.' in spread_var:
                # Split into module and property parts
                parts = spread_var.split('.')
                module_name = parts[0]  # e.g., "database"
                property_path = '.'.join(parts[1:])  # e.g., "description" or "options.list"

                # Look up the module file
                source_file = var_to_file.get(module_name)

                if source_file and Path(source_file).exists():
                    try:
                        with open(source_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # Extract the specific exported property
                        props = TypeScriptParser.extract_exported_array(content, property_path, Path(source_file))
                        if props:
                            resolved_properties.extend(props)
                            continue

                    except Exception:
                        pass

                # If not found in imports, continue to next spread
                continue

            # Original logic for simple spread variables (no property access)
            # First, check if this is a local const in the current file
            if current_content:
                local_props = TypeScriptParser.extract_exported_array(current_content, spread_var, node_path)
                if not local_props:
                    # Try without 'export' (local const arrays)
                    local_pattern = rf"const\s+{spread_var}\s*:\s*INodeProperties\[\]\s*=\s*\["
                    local_match = re.search(local_pattern, current_content)
                    if local_match:
                        # Extract using bracket counting
                        start_pos = local_match.end() - 1
                        depth = 1
                        i = start_pos + 1
                        while i < len(current_content) and depth > 0:
                            if current_content[i] == '[':
                                depth += 1
                            elif current_content[i] == ']':
                                depth -= 1
                            i += 1
                        if depth == 0:
                            array_content = current_content[start_pos+1:i-1]
                            local_props, local_spreads = TypeScriptParser._parse_property_objects(array_content)
                            # Recursively resolve local spreads
                            if local_spreads:
                                local_props.extend(TypeScriptParser._resolve_spreads(local_spreads, imports, node_path, current_content))

                if local_props:
                    resolved_properties.extend(local_props)
                    continue

            # Look up which file exports this variable
            source_file = var_to_file.get(spread_var)

            if not source_file:
                # Try to find in same directory with common naming patterns
                node_dir = node_path.parent
                potential_files = [
                    node_dir / 'descriptions' / f"{spread_var}.ts",
                    node_dir / f"{spread_var}.ts",
                    node_dir / f"descriptions/{spread_var}.ts",
                ]

                for pf in potential_files:
                    if pf.exists():
                        source_file = str(pf)
                        break

            if not source_file or not Path(source_file).exists():
                continue

            # Load the file
            try:
                source_file_path = Path(source_file)
                with open(source_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Try to extract as an array first (INodeProperties[])
                props = TypeScriptParser.extract_exported_array(content, spread_var, source_file_path)
                if props:
                    resolved_properties.extend(props)
                else:
                    # Try to extract as a single property object (INodeProperties)
                    single_prop = TypeScriptParser.extract_exported_property(content, spread_var)
                    if single_prop:
                        resolved_properties.append(single_prop)

            except Exception as e:
                # Silently continue if we can't read the file
                pass

        return resolved_properties

    @staticmethod
    def _camel_to_title_case(camel_str: str) -> str:
        """Convert camelCase or snake_case to Title Case"""
        if not camel_str:
            return ''
        # Handle snake_case
        if '_' in camel_str:
            return ' '.join(word.capitalize() for word in camel_str.split('_'))
        # Handle camelCase
        # Insert space before uppercase letters
        import re as regex_mod
        spaced = regex_mod.sub(r'([a-z])([A-Z])', r'\1 \2', camel_str)
        # Capitalize first letter and each word after space
        return ' '.join(word.capitalize() for word in spaced.split())

    @staticmethod
    def _parse_single_property(obj_str: str) -> Optional[Dict[str, Any]]:
        """Parse a single property object with maximum detail extraction"""
        prop = {}

        # Extract ALL string fields (including 'value' for options)
        string_fields = [
            'displayName', 'name', 'type', 'description',
            'placeholder', 'hint', 'action', 'tooltip',
            'extractValue', 'validateType', 'notice', 'value'
        ]
        for field in string_fields:
            value = TypeScriptParser.extract_string_value(obj_str, field)
            if value:
                prop[field] = value

        # Fallback: If displayName is missing, null, or empty but name exists, generate displayName from name
        if 'name' in prop and (not prop.get('displayName') or prop.get('displayName') == 'null'):
            prop['displayName'] = TypeScriptParser._camel_to_title_case(prop['name'])

        # Extract default value
        default_match = re.search(r"default:\s*(['\"]([^'\"]*)['\"]|`([^`]*)`|(true|false|\d+|''|{}|\[\]))", obj_str)
        if default_match:
            if default_match.group(2) is not None:
                prop['default'] = default_match.group(2)
            elif default_match.group(3) is not None:
                prop['default'] = default_match.group(3)
            elif default_match.group(4):
                val = default_match.group(4)
                if val in ['true', 'false']:
                    prop['default'] = val == 'true'
                elif val == "''":
                    prop['default'] = ''
                elif val == '{}':
                    prop['default'] = {}  # Actual empty object, not string
                elif val == '[]':
                    prop['default'] = []  # Actual empty array, not string
                else:
                    try:
                        prop['default'] = int(val)
                    except ValueError:
                        prop['default'] = val

        # Check if required
        if re.search(r"required:\s*true", obj_str):
            prop['required'] = True

        # Check for noDataExpression
        if re.search(r"noDataExpression:\s*true", obj_str):
            prop['noDataExpression'] = True

        # Parse options
        if prop.get('type') in ['options', 'collection', 'multiOptions']:
            options_match = re.search(r"options:\s*\[([\s\S]*?)\](?:,|\s*\})", obj_str)
            if options_match:
                prop['options'] = TypeScriptParser._parse_options(options_match.group(1))

        # Parse fixedCollection type with values arrays
        if prop.get('type') == 'fixedCollection':
            options_match = re.search(r"options:\s*\[([\s\S]*?)\](?:,|\s*\})", obj_str)
            if options_match:
                options_content = options_match.group(1)
                fixed_options = []
                objects, _ = TypeScriptParser._split_into_objects(options_content)
                for opt_obj_str in objects:
                    fixed_opt = {}
                    # Extract name and displayName
                    for field in ['name', 'displayName']:
                        val = TypeScriptParser.extract_string_value(opt_obj_str, field)
                        if val:
                            fixed_opt[field] = val

                    # Extract values array (these are the actual sub-properties)
                    values_match = re.search(r"values:\s*\[([\s\S]*?)\](?:,|\s*\})", opt_obj_str)
                    if values_match:
                        values_content = values_match.group(1)
                        # Parse values as full properties
                        values_props = []
                        value_objects, _ = TypeScriptParser._parse_property_objects(values_content)
                        for value_prop_dict in value_objects:
                            # Each value is a full property object
                            if value_prop_dict.get('name') or value_prop_dict.get('displayName'):
                                values_props.append(value_prop_dict)
                        if values_props:
                            fixed_opt['values'] = values_props

                    if fixed_opt:
                        fixed_options.append(fixed_opt)

                if fixed_options:
                    prop['options'] = fixed_options

        # Parse resourceLocator modes
        if prop.get('type') == 'resourceLocator':
            modes_match = re.search(r"modes:\s*\[([\s\S]*?)\](?:,|\s*\})", obj_str)
            if modes_match:
                modes_content = modes_match.group(1)
                modes = []
                objects, _ = TypeScriptParser._split_into_objects(modes_content)
                for mode_obj_str in objects:
                    mode = {}
                    for field in ['displayName', 'name', 'type', 'placeholder', 'hint', 'validation']:
                        val = TypeScriptParser.extract_string_value(mode_obj_str, field)
                        if val:
                            mode[field] = val

                    # Extract typeOptions for modes
                    type_opts_match = re.search(r"typeOptions:\s*\{", mode_obj_str)
                    if type_opts_match:
                        start_pos = type_opts_match.end()
                        depth = 1
                        i = start_pos
                        while i < len(mode_obj_str) and depth > 0:
                            if mode_obj_str[i] == '{':
                                depth += 1
                            elif mode_obj_str[i] == '}':
                                depth -= 1
                            i += 1
                        if depth == 0:
                            type_opts_content = mode_obj_str[start_pos:i-1]
                            mode['typeOptions'] = TypeScriptParser._parse_type_options(type_opts_content)

                    # Extract validation array
                    validation_match = re.search(r"validation:\s*\[([\s\S]*?)\]", mode_obj_str)
                    if validation_match:
                        validation_content = validation_match.group(1)
                        # Parse validation objects
                        validation_rules = []
                        val_objects, _ = TypeScriptParser._split_into_objects(validation_content)
                        for val_obj_str in val_objects:
                            val_rule = {}
                            for field in ['type', 'properties']:
                                val = TypeScriptParser.extract_string_value(val_obj_str, field)
                                if val:
                                    val_rule[field] = val
                            if val_rule:
                                validation_rules.append(val_rule)
                        if validation_rules:
                            mode['validation'] = validation_rules

                    if mode:
                        modes.append(mode)

                if modes:
                    prop['modes'] = modes

        # Extract typeOptions (minValue, maxValue, etc.) with full detail
        type_opts_match = re.search(r"typeOptions:\s*\{", obj_str)
        if type_opts_match:
            start_pos = type_opts_match.end()
            depth = 1
            i = start_pos
            while i < len(obj_str) and depth > 0:
                if obj_str[i] == '{':
                    depth += 1
                elif obj_str[i] == '}':
                    depth -= 1
                i += 1
            if depth == 0:
                type_opts_content = obj_str[start_pos:i-1]
                prop['typeOptions'] = TypeScriptParser._parse_type_options(type_opts_content)

        # Extract displayOptions using bracket-depth counting
        display_opts_match = re.search(r"displayOptions:\s*\{", obj_str)
        if display_opts_match:
            start_pos = display_opts_match.end()
            depth = 1
            i = start_pos
            while i < len(obj_str) and depth > 0:
                if obj_str[i] == '{':
                    depth += 1
                elif obj_str[i] == '}':
                    depth -= 1
                i += 1
            if depth == 0:
                display_opts_content = obj_str[start_pos:i-1]
                prop['displayOptions'] = TypeScriptParser._parse_display_options(display_opts_content)

        # Extract routing information (for API nodes)
        routing_match = re.search(r"routing:\s*\{([^}]+)\}", obj_str)
        if routing_match:
            prop['routing'] = routing_match.group(1)

        return prop if prop.get('name') or prop.get('displayName') else None

    @staticmethod
    def _parse_options(options_str: str) -> List[Dict[str, Any]]:
        """Parse options array with maximum detail - FULL recursive property parsing"""
        options = []
        objects, spreads = TypeScriptParser._split_into_objects(options_str)

        for obj_str in objects:
            # Use the full property parser to get ALL fields
            opt = TypeScriptParser._parse_single_property(obj_str)

            if opt:
                options.append(opt)

        return options

    @staticmethod
    def _parse_type_options(type_opts_str: str) -> Dict[str, Any]:
        """Parse typeOptions object with maximum detail"""
        type_opts = {}

        # Extract numeric values (minValue, maxValue, etc.)
        numeric_fields = ['minValue', 'maxValue', 'numberStepSize', 'numberPrecision', 'rows']
        for field in numeric_fields:
            match = re.search(rf"{field}:\s*(\d+(?:\.\d+)?)", type_opts_str)
            if match:
                type_opts[field] = float(match.group(1)) if '.' in match.group(1) else int(match.group(1))

        # Extract boolean values
        bool_fields = ['alwaysOpenEditWindow', 'multipleValues', 'password', 'editor']
        for field in bool_fields:
            match = re.search(rf"{field}:\s*(true|false)", type_opts_str)
            if match:
                type_opts[field] = match.group(1) == 'true'

        # Extract string values
        string_fields = ['loadOptionsMethod', 'filter', 'sortable', 'resizable']
        for field in string_fields:
            value = TypeScriptParser.extract_string_value(type_opts_str, field)
            if value:
                type_opts[field] = value

        return type_opts

    @staticmethod
    def _parse_display_options(display_str: str) -> Dict[str, Any]:
        """Parse displayOptions object with full detail - properly handling booleans"""
        display_opts = {}

        def parse_value(v: str):
            """Parse a value, converting booleans and numbers appropriately"""
            v = v.strip().strip("'\"")
            # Check for boolean
            if v == 'true':
                return True
            elif v == 'false':
                return False
            # Check for number
            try:
                if '.' in v:
                    return float(v)
                return int(v)
            except ValueError:
                return v

        # Parse show conditions
        show_match = re.search(r"show:\s*\{([\s\S]*?)\}", display_str)
        if show_match:
            show_content = show_match.group(1)
            show_opts = {}

            for match in re.finditer(r"(\w+):\s*\[([^\]]+)\]", show_content):
                key = match.group(1)
                values_str = match.group(2)
                values = [parse_value(v) for v in values_str.split(',')]
                show_opts[key] = values

            display_opts['show'] = show_opts

        # Parse hide conditions
        hide_match = re.search(r"hide:\s*\{([\s\S]*?)\}", display_str)
        if hide_match:
            hide_content = hide_match.group(1)
            hide_opts = {}

            for match in re.finditer(r"(\w+):\s*\[([^\]]+)\]", hide_content):
                key = match.group(1)
                values_str = match.group(2)
                values = [parse_value(v) for v in values_str.split(',')]
                hide_opts[key] = values

            display_opts['hide'] = hide_opts

        return display_opts

    @staticmethod
    def is_versioned_node(content: str, node_file: Path) -> bool:
        """
        Check if node uses VersionedNodeType pattern.

        Indicators:
        - extends VersionedNodeType
        - has V1/, V2/, v1/, v2/ subdirectories
        - uses INodeTypeBaseDescription
        - file is IN a version subdirectory (V1/, V2/, etc.)
        """
        # Check if current file is IN a version subdirectory
        if node_file.exists() and node_file.parent.name in ['V1', 'V2', 'V3', 'v1', 'v2', 'v3']:
            return True

        # Check for VersionedNodeType inheritance in content
        if 'extends VersionedNodeType' in content:
            return True

        if 'implements IVersionedNodeType' in content:
            return True

        # Check for baseDescription (strong indicator)
        if 'INodeTypeBaseDescription' in content:
            return True

        # Check for version subdirectories in same parent
        if node_file.exists() and node_file.parent.exists():
            parent_dir = node_file.parent
            version_dirs = [d for d in parent_dir.iterdir()
                          if d.is_dir() and d.name in ['V1', 'V2', 'V3', 'v1', 'v2', 'v3']]
            if version_dirs:
                return True

        return False

    @staticmethod
    def extract_base_description(content: str) -> Dict[str, Any]:
        """
        Extract baseDescription from main versioned node file.

        Pattern:
        const baseDescription: INodeTypeBaseDescription = {
            displayName: 'Slack',
            name: 'slack',
            icon: 'file:slack.svg',
            group: ['output'],
            ...
        };
        """
        base_desc = {}

        # Find baseDescription object
        pattern = r'const\s+baseDescription\s*:\s*INodeTypeBaseDescription\s*=\s*\{'
        match = re.search(pattern, content)

        if not match:
            return base_desc

        # Extract object using bracket-depth counting
        start_pos = match.end() - 1  # Start at opening {
        depth = 1
        i = start_pos + 1

        while i < len(content) and depth > 0:
            if content[i] == '{':
                depth += 1
            elif content[i] == '}':
                depth -= 1
            i += 1

        if depth != 0:
            return base_desc

        obj_content = content[start_pos:i]

        # Extract displayName
        display_match = re.search(r"""displayName:\s*['"](.*?)['"]""", obj_content)
        if display_match:
            base_desc['displayName'] = display_match.group(1)

        # Extract name
        name_match = re.search(r"""name:\s*['"](.*?)['"]""", obj_content)
        if name_match:
            base_desc['name'] = name_match.group(1)

        # Extract icon (string format)
        icon_match = re.search(r"""icon:\s*['"](.*?)['"]""", obj_content)
        if icon_match:
            base_desc['icon'] = icon_match.group(1)
        else:
            # Handle icon object format { light: '...', dark: '...' }
            icon_obj_match = re.search(r"icon:\s*\{([^}]+)\}", obj_content)
            if icon_obj_match:
                icon_obj_str = icon_obj_match.group(1)
                # Try to extract light or default icon path
                light_match = re.search(r"""light:\s*['"](.*?)['"]""", icon_obj_str)
                if light_match:
                    base_desc['icon'] = {'light': light_match.group(1)}
                    # Also try dark
                    dark_match = re.search(r"""dark:\s*['"](.*?)['"]""", icon_obj_str)
                    if dark_match:
                        base_desc['icon']['dark'] = dark_match.group(1)

        # Extract group (array)
        group_match = re.search(r"""group:\s*\[([^\]]+)\]""", obj_content)
        if group_match:
            group_str = group_match.group(1)
            # Extract quoted strings
            groups = re.findall(r"""['"](.*?)['"]""", group_str)
            if groups:
                base_desc['group'] = groups

        # Extract subtitle
        subtitle_match = re.search(r"""subtitle:\s*['"](.*?)['"]""", obj_content)
        if subtitle_match:
            base_desc['subtitle'] = subtitle_match.group(1)

        # Extract description
        desc_match = re.search(r"""description:\s*['"](.*?)['"]""", obj_content)
        if desc_match:
            base_desc['description'] = desc_match.group(1)

        # Extract outputNames (for branching nodes like If, Switch)
        output_names_match = re.search(r"""outputNames:\s*\[([^\]]+)\]""", obj_content)
        if output_names_match:
            output_names_str = output_names_match.group(1)
            # Extract quoted strings
            output_names = re.findall(r"""['"](.*?)['"]""", output_names_str)
            if output_names:
                base_desc['outputNames'] = output_names

        return base_desc


class NodeExtractor:
    """Extract comprehensive node information with maximum detail for AI training"""

    def __init__(self, node_name: str):
        self.node_name = node_name
        self.node_dir = None
        self.main_file = None
        self.files = {}
        self.versioned_base_file: Optional[Path] = None
        self.node_info = {
            'icon': '',
            'defaults': {},
            'inputs': [],
            'outputs': [],
            'group': [],  # Fixed: should be array, not string
            'authentication': [],
            'loadOptionsMethods': [],
            'eventTriggerDescription': '',
            'activationMessage': '',
            'triggerPanel': {},
            'supportsCORS': False,
        }
        self.metadata = {}
        self.properties = []
        self.operations = []
        self.operations_by_resource = {}
        self.credentials = []
        self.workflow_examples = []
        self.expressions = []
        self.validation_rules = {}
        self.type_info = {}
        self.api_patterns = {}
        self.ui_elements = {
            'notices': [],
            'tooltips': [],
            'placeholders': [],
            'hints': []
        }

    def set_node_file(self, node_file_path: Path) -> bool:
        """Set the node file directly from a path"""
        if not node_file_path.exists():
            print(f" Error: File does not exist: {node_file_path}")
            return False

        self.main_file = node_file_path
        self.node_dir = self.main_file.parent
        self.node_name = self.main_file.stem.replace('.node', '')

        return True

    def _resolve_versioned_main(self, content: str) -> Optional[Path]:
        """If node is versioned, resolve the default version file."""
        # First check for VersionedNodeType pattern
        if 'VersionedNodeType' in content:
            default_version_match = re.search(r'defaultVersion:\s*(\d+(?:\.\d+)?)', content)
            if not default_version_match:
                return None

            default_version = default_version_match.group(1)

            version_map = {}
            for match in re.finditer(r'(\d+(?:\.\d+)?)\s*:\s*new\s+(\w+)\s*\(', content):
                version_map[match.group(1)] = match.group(2)

            target_class = version_map.get(default_version)
            if not target_class:
                return None

            import_map = {}
            for match in re.finditer(r"import\s+\{\s*([^}]+)\s*\}\s+from\s+['\"]([^'\"]+)['\"]", content):
                names = [name.strip() for name in match.group(1).split(',')]
                source = match.group(2)
                for name in names:
                    import_map[name] = source

            rel_path = import_map.get(target_class)
            if not rel_path:
                return None

            if rel_path.startswith('./'):
                rel_path = rel_path[2:]

            candidate = (self.main_file.parent / rel_path)

            potential_paths = [candidate]
            if not candidate.name.endswith('.ts'):
                potential_paths.append(candidate.parent / f"{candidate.name}.ts")
            if candidate.suffix and candidate.suffix != '.ts':
                potential_paths.append(candidate.with_suffix('.ts'))

            for path in potential_paths:
                if path.exists():
                    return path

        # Check for versionDescription import pattern (e.g., PostgresV2)
        version_desc_match = re.search(r"import\s+\{\s*versionDescription\s*\}\s+from\s+['\"]([^'\"]+)['\"]", content)
        if version_desc_match:
            rel_path = version_desc_match.group(1)
            if rel_path.startswith('./'):
                rel_path = rel_path[2:]

            candidate = (self.main_file.parent / rel_path)

            potential_paths = [candidate]
            if not candidate.name.endswith('.ts'):
                potential_paths.append(candidate.parent / f"{candidate.name}.ts")
            if candidate.suffix and candidate.suffix != '.ts':
                potential_paths.append(candidate.with_suffix('.ts'))

            for path in potential_paths:
                if path.exists():
                    return path

        return None

    def find_parent_node_file(self) -> Optional[Path]:
        """
        Find parent .node.ts file for versioned nodes.

        For a file in V1/, V2/, v1/, v2/ subdirectory, find the parent file
        that contains baseDescription.

        Example:
        - Input: /path/to/Slack/V2/SlackV2.node.ts
        - Output: /path/to/Slack/Slack.node.ts
        """
        if not self.main_file:
            return None

        # Check if we're in a version subdirectory
        parent_dir = self.main_file.parent
        if parent_dir.name not in ['V1', 'V2', 'V3', 'v1', 'v2', 'v3']:
            # Not in a version subdirectory, check if this file IS the parent
            content = self.main_file.read_text(encoding='utf-8', errors='ignore')
            if 'INodeTypeBaseDescription' in content or 'extends VersionedNodeType' in content:
                return self.main_file
            return None

        # We're in a version subdirectory, find parent
        grandparent_dir = parent_dir.parent

        # Look for main .node.ts file in grandparent directory
        for candidate in grandparent_dir.glob('*.node.ts'):
            # Skip version subdirectories
            if candidate.parent != grandparent_dir:
                continue

            # Check if this file has baseDescription
            try:
                content = candidate.read_text(encoding='utf-8', errors='ignore')
                if 'INodeTypeBaseDescription' in content or 'extends VersionedNodeType' in content:
                    return candidate
            except:
                continue

        return None

    @staticmethod
    def _extend_unique_operations(target: List[Dict[str, Any]], operations: List[Dict[str, Any]]):
        """Append operations ensuring uniqueness by value or name."""
        seen_keys = set()
        for existing in target:
            key = existing.get('value') or existing.get('name')
            if key:
                seen_keys.add(key)

        for op in operations:
            key = op.get('value') or op.get('name')
            if key and key in seen_keys:
                continue
            target.append(op)
            if key:
                seen_keys.add(key)

    def _merge_credentials(self, new_credentials: List[Dict[str, Any]]):
        """Merge credential definitions while preventing duplicates."""
        existing_names = {cred.get('name') for cred in self.credentials if cred.get('name')}
        for cred in new_credentials:
            name = cred.get('name')
            if not name or name in existing_names:
                continue
            self.credentials.append(cred)
            existing_names.add(name)

    @staticmethod
    def _deduplicate_properties(properties: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Deduplicate properties that were extracted multiple times.

        Deduplication key: (name, displayOptions)
        - Properties with same name but different displayOptions are kept (intentional duplicates)
        - For exact duplicates, prefer the one WITH _source field (better tracking)

        Args:
            properties: List of property dictionaries

        Returns:
            Deduplicated list of properties
        """
        import json
        seen = {}  # key: (name, display_options_str) -> property dict

        for prop in properties:
            name = prop.get('name')
            if not name:
                continue

            # Create a stable key from displayOptions
            display_opts = prop.get('displayOptions', {})
            # Convert to JSON string for hashable key (handles nested structures)
            try:
                display_opts_str = json.dumps(display_opts, sort_keys=True) if display_opts else ""
            except (TypeError, ValueError):
                # If JSON serialization fails, use str() as fallback
                display_opts_str = str(display_opts)

            key = (name, display_opts_str)

            if key in seen:
                # Duplicate found - prefer the one with _source field
                existing = seen[key]
                if prop.get('_source') and not existing.get('_source'):
                    # Current has _source, existing doesn't - replace
                    seen[key] = prop
                # Otherwise keep existing (either both have _source or existing has it)
            else:
                # First occurrence of this property
                seen[key] = prop

        return list(seen.values())

    def find_node(self) -> bool:
        """Find the node directory and files"""
        nodes_dir = N8N_REPO / "packages" / "nodes-base" / "nodes"

        if not nodes_dir.exists():
            print(f" Error: Cannot find nodes directory at {nodes_dir}")
            return False

        matches = list(nodes_dir.glob(f"**/{self.node_name}.node.ts"))
        if not matches:
            matches = list(nodes_dir.glob(f"**/*{self.node_name}*.node.ts"))

        if not matches:
            print(f" Node '{self.node_name}' not found")
            return False

        return self.set_node_file(matches[0])

    def load_files(self) -> bool:
        """Load all node-related files"""
        metadata: Dict[str, Any] = {}

        with open(self.main_file, 'r', encoding='utf-8') as f:
            main_content = f.read()

        base_metadata_path = self.main_file.with_suffix('.json')
        if base_metadata_path.exists():
            try:
                with open(base_metadata_path, 'r', encoding='utf-8') as meta_file:
                    metadata.update(json.load(meta_file))
            except json.JSONDecodeError:
                pass

        version_file = self._resolve_versioned_main(main_content)
        if version_file:
            self.versioned_base_file = self.main_file
            self.main_file = version_file
            self.node_dir = version_file.parent
            with open(version_file, 'r', encoding='utf-8') as version_f:
                main_content = version_f.read()

            version_metadata_path = version_file.with_suffix('.json')
            if version_metadata_path.exists():
                try:
                    with open(version_metadata_path, 'r', encoding='utf-8') as meta_file:
                        metadata.update(json.load(meta_file))
                except json.JSONDecodeError:
                    pass
        else:
            # Ensure node_dir points to main file parent when not versioned
            self.node_dir = self.main_file.parent

        self.metadata = metadata
        self.files['main'] = main_content

        # Load .node.json metadata for final main file if not already captured
        final_metadata_path = self.main_file.with_suffix('.json')
        if final_metadata_path.exists() and not metadata:
            try:
                with open(final_metadata_path, 'r', encoding='utf-8') as meta_file:
                    self.metadata = json.load(meta_file)
            except json.JSONDecodeError:
                pass

        # Check for imported properties
        properties_import_match = re.search(r"properties:\s*(\w+)", self.files['main'])
        if properties_import_match:
            prop_var = properties_import_match.group(1)
            import_pattern = rf"import\s+\{{\s*{prop_var}\s*\}}\s+from\s+['\"](\.[^'\"]+)['\"]"
            import_match = re.search(import_pattern, self.files['main'])
            if import_match:
                import_path = import_match.group(1)
                if import_path.startswith('./'):
                    import_path = import_path[2:]

                for ext in ['.ts', '']:
                    file_path = self.node_dir / f"{import_path}{ext}"
                    if file_path.exists():
                        with open(file_path, 'r', encoding='utf-8') as f:
                            self.files['properties_file'] = f.read()
                        break

        # Load operation files
        import_pattern = r"import\s+\*\s+as\s+(\w+)\s+from\s+['\"](\.[^'\"]+)['\"]"
        imports = re.findall(import_pattern, self.files['main'])

        for import_name, import_path in imports:
            if import_path.startswith('./'):
                import_path = import_path[2:]

            for ext in ['.ts', '']:
                file_path = self.node_dir / f"{import_path}{ext}"
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        self.files[import_name] = f.read()
                    break

        # Load action files
        actions_dir = self.node_dir / "actions"
        if actions_dir.exists():
            for action_file in actions_dir.rglob("*.ts"):
                action_key = None
                if action_file.name.endswith('.operation.ts'):
                    action_key = action_file.stem.replace('.operation', '')
                elif action_file.name.endswith('.resource.ts'):
                    action_key = action_file.stem

                if action_key:
                    with open(action_file, 'r', encoding='utf-8') as f:
                        self.files[action_key] = f.read()

        # Load GenericFunctions.ts if exists (contains API patterns)
        generic_funcs = self.node_dir / "GenericFunctions.ts"
        if generic_funcs.exists():
            with open(generic_funcs, 'r', encoding='utf-8') as f:
                self.files['generic_functions'] = f.read()

        return True

    def extract_info(self):
        """Extract all node information with maximum detail - FIXED VERSION"""
        main = self.files['main']

        # === VERSIONED NODE BASE DESCRIPTION ===
        # Check if this is a VersionedNodeType node and extract baseDescription from parent file
        if TypeScriptParser.is_versioned_node(main, self.main_file):
            parent_file = self.find_parent_node_file()
            if parent_file and parent_file.exists():
                try:
                    parent_content = parent_file.read_text(encoding='utf-8', errors='ignore')
                    base_desc = TypeScriptParser.extract_base_description(parent_content)

                    # Apply baseDescription fields (these provide defaults for versioned nodes)
                    if base_desc.get('displayName'):
                        self.node_info['displayName'] = base_desc['displayName']
                    if base_desc.get('name'):
                        # Store for potential use, but may be overridden by metadata
                        pass  # We'll let metadata override if present
                    if base_desc.get('icon'):
                        self.node_info['icon'] = base_desc['icon']
                    if base_desc.get('group'):
                        self.node_info['group'] = base_desc['group']
                    if base_desc.get('subtitle'):
                        self.node_info['subtitle'] = base_desc['subtitle']
                    if base_desc.get('description') and not self.node_info.get('description'):
                        self.node_info['description'] = base_desc['description']
                    if base_desc.get('outputNames'):
                        self.node_info['outputNames'] = base_desc['outputNames']

                except Exception as e:
                    # If baseDescription extraction fails, continue with standard extraction
                    pass

        # === BASIC INFO ===
        # Extract from INodeTypeDescription object, not from random places
        # Find the description object
        desc_match = re.search(r'description:\s*INodeTypeDescription\s*=\s*\{', main)
        if desc_match:
            # Extract displayName, name within the description block
            desc_start = desc_match.end()
            # Look for displayName in next 5000 chars
            desc_block = main[desc_start:desc_start+5000]
            display_name = TypeScriptParser.extract_string_value(desc_block, 'displayName')
            node_name = TypeScriptParser.extract_string_value(desc_block, 'name')
            description = TypeScriptParser.extract_string_value(desc_block, 'description')
            subtitle = TypeScriptParser.extract_string_value(desc_block, 'subtitle')

            # Only set if not already set from baseDescription
            if not self.node_info.get('displayName'):
                self.node_info['displayName'] = display_name or self.node_name
            if not self.node_info.get('name'):
                self.node_info['name'] = node_name or self.node_name
            if not self.node_info.get('description'):
                self.node_info['description'] = description or ''
            if not self.node_info.get('subtitle'):
                self.node_info['subtitle'] = subtitle or ''
        else:
            # Fallback to searching whole file (only if not set from baseDescription)
            if not self.node_info.get('displayName'):
                self.node_info['displayName'] = TypeScriptParser.extract_string_value(main, 'displayName') or self.node_name
            if not self.node_info.get('name'):
                self.node_info['name'] = TypeScriptParser.extract_string_value(main, 'name') or self.node_name
            if not self.node_info.get('description'):
                self.node_info['description'] = TypeScriptParser.extract_string_value(main, 'description') or ''
            if not self.node_info.get('subtitle'):
                self.node_info['subtitle'] = TypeScriptParser.extract_string_value(main, 'subtitle') or ''

        # Override name from metadata if available
        if self.metadata.get('node'):
            metadata_name = self.metadata['node'].split('.')[-1]
            if metadata_name:
                self.node_info['name'] = metadata_name

        # === ICON EXTRACTION (handle both string and object) ===
        # Only extract if not already set from baseDescription
        if not self.node_info.get('icon'):
            # Try string first
            icon_match = re.search(r"""icon:\s*["']([^"']*?)["']""", main)
            if icon_match:
                self.node_info['icon'] = icon_match.group(1)
            else:
                # Try object (light/dark themes)
                icon_obj = TypeScriptParser.extract_object_value(main, 'icon')
                if icon_obj:
                    self.node_info['icon'] = icon_obj
                else:
                    self.node_info['icon'] = ''

        # === ICON COLOR ===
        icon_color = TypeScriptParser.extract_string_value(main, 'iconColor')
        if icon_color:
            self.node_info['iconColor'] = icon_color

        # === DEFAULTS (parse as object) ===
        defaults_match = re.search(r"defaults:\s*\{", main)
        if defaults_match:
            start_pos = defaults_match.end() - 1
            depth = 1
            i = start_pos + 1
            while i < len(main) and depth > 0:
                if main[i] == '{':
                    depth += 1
                elif main[i] == '}':
                    depth -= 1
                i += 1
            if depth == 0:
                defaults_str = main[start_pos:i]
                # Try to extract name field from defaults
                defaults_obj = {}
                name_match = re.search(r"""name:\s*['"](.*?)['"]""", defaults_str)
                if name_match:
                    defaults_obj['name'] = name_match.group(1)
                self.node_info['defaults'] = defaults_obj if defaults_obj else defaults_str

        # === GROUP (array of strings) ===
        # Only extract if not already set from baseDescription
        if not self.node_info.get('group') or len(self.node_info.get('group', [])) == 0:
            group_values = TypeScriptParser.extract_array_value(main, 'group')
            if group_values:
                self.node_info['group'] = group_values
            else:
                # Try to extract NodeGroup enum values
                group_match = re.search(r"group:\s*\[([^\]]+)\]", main)
                if group_match:
                    group_str = group_match.group(1)
                    # Extract enum values like NodeGroup.Input, NodeGroup.Output
                    group_enums = re.findall(r"NodeGroup\.(\w+)", group_str)
                    if group_enums:
                        # Convert enum values to lowercase for consistency
                        self.node_info['group'] = [g.lower() for g in group_enums]
                    else:
                        # Fallback: try to extract as quoted strings
                        quoted_values = re.findall(r"['\"]([^'\"]+)['\"]", group_str)
                        if quoted_values:
                            self.node_info['group'] = [g.strip() for val in quoted_values for g in val.split(',') if g.strip()]
                        else:
                            self.node_info['group'] = []
                else:
                    self.node_info['group'] = []

        # === INPUTS (array of connection types) ===
        # Only extract if not already set from versionDescription
        if not self.node_info.get('inputs'):
            inputs_match = re.search(r"inputs:\s*\[([^\]]+)\]", main)
            if inputs_match:
                inputs_str = inputs_match.group(1)
                input_types = re.findall(r"NodeConnectionTypes\.(\w+)", inputs_str)
                self.node_info['inputs'] = input_types if input_types else []
            else:
                self.node_info['inputs'] = []

        # === OUTPUTS (array of connection types) ===
        # Only extract if not already set from versionDescription
        if not self.node_info.get('outputs'):
            outputs_match = re.search(r"outputs:\s*\[([^\]]+)\]", main)
            if outputs_match:
                outputs_str = outputs_match.group(1)
                output_types = re.findall(r"NodeConnectionTypes\.(\w+)", outputs_str)
                self.node_info['outputs'] = output_types if output_types else []
            else:
                self.node_info['outputs'] = []

        # === VERSION (can be single number or array) ===
        version_array_match = re.search(r"version:\s*\[([^\]]+)\]", main)
        if version_array_match:
            version_str = version_array_match.group(1)
            versions = [v.strip() for v in version_str.split(',')]
            self.node_info['version'] = versions
        else:
            version_match = re.search(r"version:\s*(\d+(?:\.\d+)?)", main)
            if version_match:
                self.node_info['version'] = version_match.group(1)

        # === USABLE AS TOOL ===
        usable_as_tool_match = re.search(r"usableAsTool:\s*(true|false)", main)
        if usable_as_tool_match:
            self.node_info['usableAsTool'] = usable_as_tool_match.group(1) == 'true'

        # === LOAD OPTIONS METHODS (from methods.loadOptions object) ===
        methods_match = re.search(r"methods\s*=\s*\{", main)
        if methods_match:
            start_pos = methods_match.end() - 1
            depth = 1
            i = start_pos + 1
            while i < len(main) and depth > 0:
                if main[i] == '{':
                    depth += 1
                elif main[i] == '}':
                    depth -= 1
                i += 1
            if depth == 0:
                methods_content = main[start_pos:i]
                # Extract loadOptions method names (only method definitions, not calls)
                load_opts_match = re.search(r"loadOptions:\s*\{([\s\S]*?)\}", methods_content)
                if load_opts_match:
                    load_opts_content = load_opts_match.group(1)
                    # Match method definitions: either 'methodName(' or 'methodName:' at start of line or after comma
                    # This avoids capturing function calls within method bodies
                    method_names = re.findall(r"(?:^|,)\s*(?:async\s+)?(\w+)\s*(?:\(|:)", load_opts_content, re.MULTILINE)
                    if method_names:
                        self.node_info['loadOptionsMethods'] = method_names

                # Extract listSearch method names (only method definitions, not calls)
                list_search_match = re.search(r"listSearch:\s*\{([\s\S]*?)\}", methods_content)
                if list_search_match:
                    list_search_content = list_search_match.group(1)
                    # Match method definitions: either 'methodName(' or 'methodName:' at start of line or after comma
                    # This avoids capturing function calls within method bodies
                    search_method_names = re.findall(r"(?:^|,)\s*(?:async\s+)?(\w+)\s*(?:\(|:)", list_search_content, re.MULTILINE)
                    if search_method_names:
                        self.node_info['listSearchMethods'] = search_method_names

        # === DETECT NODE TYPE (regular, trigger, webhook) ===
        node_type = 'regular'
        group_list = self.node_info.get('group', [])
        if group_list and 'trigger' in group_list:
            node_type = 'webhook' if 'webhook' in self.node_name.lower() else 'trigger'
        self.node_info['node_type'] = node_type

        # === POLLING FLAG (for trigger nodes) ===
        if node_type in ['trigger', 'webhook']:
            polling_match = re.search(r"polling:\s*(true|false)", main)
            if polling_match:
                self.node_info['polling'] = polling_match.group(1) == 'true'

        # === TRIGGER-SPECIFIC PROPERTIES ===
        if node_type in ['trigger', 'webhook']:
            event_trigger = TypeScriptParser.extract_string_value(main, 'eventTriggerDescription')
            if event_trigger:
                self.node_info['eventTriggerDescription'] = event_trigger

            activation_msg = TypeScriptParser.extract_string_value(main, 'activationMessage')
            if activation_msg:
                self.node_info['activationMessage'] = activation_msg

            trigger_panel_obj = TypeScriptParser.extract_object_value(main, 'triggerPanel')
            if trigger_panel_obj:
                self.node_info['triggerPanel'] = trigger_panel_obj

            supports_cors_match = re.search(r"supportsCORS:\s*(true|false)", main)
            if supports_cors_match:
                self.node_info['supportsCORS'] = supports_cors_match.group(1) == 'true'

            # Extract webhook configuration
            webhooks_match = re.search(r"webhooks:\s*\[", main)
            if webhooks_match:
                start_pos = webhooks_match.end()
                depth = 1
                i = start_pos
                while i < len(main) and depth > 0:
                    if main[i] == '[':
                        depth += 1
                    elif main[i] == ']':
                        depth -= 1
                    i += 1
                if depth == 0:
                    webhooks_content = main[start_pos:i-1]
                    # Parse webhook objects
                    webhook_objs = []
                    for obj_str in re.finditer(r"\{([^\}]*?)\}", webhooks_content):
                        webhook_obj = {}
                        obj_content = obj_str.group(1)
                        # Extract webhook fields
                        for field in ['name', 'httpMethod', 'responseMode', 'path']:
                            val = TypeScriptParser.extract_string_value(obj_content, field)
                            if val:
                                webhook_obj[field] = val
                        if webhook_obj:
                            webhook_objs.append(webhook_obj)
                    if webhook_objs:
                        self.node_info['webhooks'] = webhook_objs

        # === PROPERTIES EXTRACTION ===
        # Check for versionDescription first (versioned nodes pattern)
        version_desc_file = TypeScriptParser.find_version_description_file(self.main_file)
        if version_desc_file and version_desc_file.exists():
            try:
                version_desc_content = version_desc_file.read_text(encoding='utf-8', errors='ignore')

                # Store versionDescription content for later metadata extraction
                # (will be applied at the end of extract_info to ensure it overrides everything)
                self._version_desc_content = version_desc_content

                # Extract properties from versionDescription
                self.properties = TypeScriptParser.extract_properties_array(version_desc_content, version_desc_file)
                if len(self.properties) > 0:
                    print(f"  Found versionDescription with {len(self.properties)} properties")
            except Exception as e:
                print(f"  Warning: Could not load versionDescription: {e}")
                # Fall back to standard extraction
                self.properties = []

        # If no properties from versionDescription, try standard extraction
        if len(self.properties) == 0:
            # Extract properties (pass node path for spread operator resolution)
            if 'properties_file' in self.files:
                self.properties = TypeScriptParser.extract_properties_array(self.files['properties_file'], self.main_file)
            else:
                self.properties = TypeScriptParser.extract_properties_array(main, self.main_file)

        # Parse imports to find description files (for modular nodes)
        node_file = self.node_dir / f"{self.node_name}.node.ts"
        imports = TypeScriptParser.parse_imports(main, node_file)

        description_credentials: List[Dict[str, Any]] = []
        description_metadata: Dict[str, Any] = {}  # Store metadata from description imports

        if imports:
            print(f"  Found {len(imports)} description files")

            for desc_file_path, imported_names in imports.items():
                try:
                    with open(desc_file_path, 'r', encoding='utf-8') as f:
                        desc_content = f.read()

                    for array_name in imported_names:
                        resource_name = Path(desc_file_path).stem.replace('Description', '')
                        # Pass file path for spread resolution
                        array_props = TypeScriptParser.extract_exported_array(desc_content, array_name, Path(desc_file_path))
                        if array_props:
                            for prop in array_props:
                                prop['_source'] = f"{resource_name}_{array_name}"
                            self.properties.extend(array_props)
                            print(f"    Loaded {len(array_props)} properties from {array_name}")
                        else:
                            desc_data = TypeScriptParser.extract_description_object(desc_content, array_name)
                            desc_props = desc_data.get('properties', [])
                            if desc_props:
                                for prop in desc_props:
                                    prop['_source'] = f"{resource_name}_{array_name}"
                                self.properties.extend(desc_props)
                                print(f"    Loaded {len(desc_props)} properties from {array_name}")

                            if desc_data.get('credentials'):
                                description_credentials.extend(desc_data['credentials'])

                            # Store metadata from description object (inputs, outputs, group, icon, etc.)
                            if array_name == 'description':  # Only for main description imports
                                for key in ['inputs', 'outputs', 'group', 'icon', 'displayName', 'subtitle', 'description', 'usableAsTool']:
                                    if key in desc_data:
                                        description_metadata[key] = desc_data[key]
                                if description_metadata:
                                    print(f"    Extracted metadata from {array_name}: {', '.join(description_metadata.keys())}")

                except Exception as e:
                    print(f"    Warning: Could not load {desc_file_path}: {e}")

        # Extract from operation files
        for file_name, content in self.files.items():
            if file_name not in ['main', 'properties_file', 'generic_functions']:
                file_props = TypeScriptParser.extract_properties_array(content, self.main_file)
                if file_props:
                    for prop in file_props:
                        prop['_source'] = file_name
                        display_opts = prop.get('displayOptions', {})
                        if display_opts and 'show' in display_opts:
                            operation_vals = display_opts['show'].get('operation', [])
                            if operation_vals:
                                prop['_operation'] = operation_vals[0] if operation_vals else None
                    self.properties.extend(file_props)

        # Deduplicate properties (fixes issue where properties are extracted via both
        # spread resolution and import parsing)
        self.properties = self._deduplicate_properties(self.properties)

        # Extract credentials
        self.operations_by_resource = {}
        all_operations: List[Dict[str, Any]] = []

        for prop in self.properties:
            if prop.get('name') == 'operation' and prop.get('type') == 'options':
                operations = copy.deepcopy(prop.get('options', []))
                display_opts = prop.get('displayOptions', {}).get('show', {})
                resource_values = display_opts.get('resource', [])

                if resource_values:
                    for resource_val in resource_values:
                        existing_ops = self.operations_by_resource.setdefault(resource_val, [])
                        self._extend_unique_operations(existing_ops, copy.deepcopy(operations))
                else:
                    self._extend_unique_operations(all_operations, operations)

        if all_operations:
            self.operations = all_operations
        elif self.operations_by_resource:
            aggregated_operations: List[Dict[str, Any]] = []
            for resource_ops in self.operations_by_resource.values():
                self._extend_unique_operations(aggregated_operations, copy.deepcopy(resource_ops))
            self.operations = aggregated_operations
        else:
            self.operations = []

        self.credentials = CredentialExtractor.extract_credentials(main)
        if description_credentials:
            self._merge_credentials(description_credentials)

        # Filter out empty credential objects
        if self.credentials:
            valid_credentials = [cred for cred in self.credentials if cred and cred.get('name')]
            self.node_info['authentication'] = copy.deepcopy(valid_credentials) if valid_credentials else []
        else:
            self.node_info['authentication'] = []

        # Extract workflow examples
        node_type = f"n8n-nodes-base.{self.node_info['name']}"
        workflow_files = WorkflowParser.find_workflow_files(self.node_dir)
        for wf_file in workflow_files:
            examples = WorkflowParser.parse_workflow(wf_file, node_type)
            self.workflow_examples.extend(examples)

        # Extract expressions from all files
        for content in self.files.values():
            self.expressions.extend(ExpressionExtractor.find_expressions(content))

        # Extract validation rules and type info from properties
        for prop in self.properties:
            if prop.get('name'):
                # Validation rules
                rules = ValidationExtractor.extract_validation_rules(prop)
                if rules:
                    self.validation_rules[prop['name']] = rules

                # Type information
                type_info = TypeInfoExtractor.extract_type_info(prop)
                self.type_info[prop['name']] = type_info

                # UI elements
                if prop.get('type') == 'notice':
                    self.ui_elements['notices'].append({
                        'name': prop.get('name'),
                        'text': prop.get('displayName'),
                        'conditions': prop.get('displayOptions')
                    })
                if prop.get('tooltip'):
                    self.ui_elements['tooltips'].append({
                        'field': prop.get('name'),
                        'text': prop.get('tooltip')
                    })
                if prop.get('placeholder'):
                    self.ui_elements['placeholders'].append({
                        'field': prop.get('name'),
                        'text': prop.get('placeholder')
                    })
                if prop.get('hint'):
                    self.ui_elements['hints'].append({
                        'field': prop.get('name'),
                        'text': prop.get('hint')
                    })

        # Extract API patterns if generic functions exist
        if 'generic_functions' in self.files:
            self.api_patterns = APIPatternExtractor.find_api_patterns(self.files['generic_functions'])

        # === DESCRIPTION IMPORT METADATA APPLICATION ===
        # Apply metadata from description imports if main file doesn't have these fields
        # (for nodes like TheHiveProject, MicrosoftOutlookV2 that import description from './actions/node.description')
        if description_metadata:
            for key in ['inputs', 'outputs', 'group', 'icon', 'displayName', 'subtitle', 'description', 'usableAsTool']:
                if key in description_metadata and not self.node_info.get(key):
                    self.node_info[key] = description_metadata[key]
                    print(f"   Applied {key} from description import: {description_metadata[key]}")

        # === FINAL VERSIONDESCRIPTION METADATA OVERRIDE ===
        # Apply versionDescription metadata LAST to ensure it overrides everything else
        if hasattr(self, '_version_desc_content') and self._version_desc_content:
            # Inputs
            inputs_match = re.search(r"inputs:\s*\[([^\]]+)\]", self._version_desc_content)
            if inputs_match:
                inputs_str = inputs_match.group(1)
                input_types = re.findall(r"NodeConnectionTypes\.(\w+)", inputs_str)
                if input_types:
                    self.node_info['inputs'] = input_types

            # Outputs
            outputs_match = re.search(r"outputs:\s*\[([^\]]+)\]", self._version_desc_content)
            if outputs_match:
                outputs_str = outputs_match.group(1)
                output_types = re.findall(r"NodeConnectionTypes\.(\w+)", outputs_str)
                if output_types:
                    self.node_info['outputs'] = output_types

            # Group
            group_match = re.search(r"group:\s*\[([^\]]+)\]", self._version_desc_content)
            if group_match:
                group_str = group_match.group(1)
                groups = re.findall(r"""['"](.*?)['"]""", group_str)
                if groups:
                    self.node_info['group'] = groups

            # Icon
            icon_match = re.search(r"""icon:\s*['"](.*?)['"]""", self._version_desc_content)
            if icon_match:
                self.node_info['icon'] = icon_match.group(1)

    def generate_markdown(self) -> str:
        """Generate comprehensive markdown with maximum detail"""
        md = []

        # Frontmatter
        slug = self.node_info['name'].lower().replace(' ', '-')
        today = date.today().strftime('%Y-%m-%d')

        md.append("---")
        md.append(f"title: \"Node: {self.node_info['displayName']}\"")
        md.append(f"slug: \"node-{slug}\"")
        md.append(f"version: \"{self.node_info.get('version', '1.0')}\"")
        md.append(f"updated: \"{today}\"")
        md.append(f"summary: \"{self.node_info['description']}\"")
        md.append(f"node_type: \"{self.node_info.get('node_type', 'regular')}\"")
        if self.node_info.get('group'):
            md.append(f"group: \"{self.node_info['group']}\"")
        md.append("---")
        md.append("")

        # Title
        md.append(f"# Node: {self.node_info['displayName']}")
        md.append("")

        # Purpose
        md.append(f"**Purpose.** {self.node_info['description']}")
        if self.node_info.get('subtitle'):
            md.append(f"**Subtitle.** {self.node_info['subtitle']}")
        md.append("")

        md.append("")

        # Node Details
        md.append("---")
        md.append("")
        md.append("## Node Details")
        md.append("")
        md.append(f"- **Icon:** `{self.node_info.get('icon')}`")
        md.append(f"- **Group:** `{self.node_info.get('group')}`")
        md.append(f"- **Inputs:** `{self.node_info.get('inputs')}`")
        md.append(f"- **Outputs:** `{self.node_info.get('outputs')}`")
        md.append("")

        # Authentication
        if self.node_info.get('authentication'):
            md.append("---")
            md.append("")
            md.append("## Authentication")
            md.append("")
            for auth in self.node_info['authentication']:
                name = auth.get('name', 'N/A')
                description = auth.get('description', 'N/A')
                md.append(f"- **{name}**: {description}")
            md.append("")

        md.append("")

        # Trigger Properties
        if self.node_info['node_type'] in ['trigger', 'webhook']:
            md.append("---")
            md.append("")
            md.append("## Trigger Properties")
            md.append("")
            md.append(f"- **Event Trigger Description:** {self.node_info.get('eventTriggerDescription')}")
            md.append(f"- **Activation Message:** {self.node_info.get('activationMessage')}")
            md.append(f"- **Supports CORS:** {self.node_info.get('supportsCORS')}")
            if self.node_info.get('triggerPanel'):
                md.append("- **Trigger Panel:**")
                md.append("```json")
                md.append(json.dumps(self.node_info['triggerPanel'], indent=2))
                md.append("```")
            md.append("")
        # UI Tips & Notices
        md.append("---")
        md.append("")
        md.append("## UI Tips & Notices")
        md.append("")

        # Global expression tip
        md.append("**Expression Mode Tip:** When using expressions (switching from Fixed to Expression mode), n8n displays:")
        md.append("")
        md.append("> Anything inside `{{ }}` is JavaScript. [Learn more](https://docs.n8n.io/code-examples/expressions/)")
        md.append("")

        # Node-specific notices
        if self.ui_elements['notices']:
            md.append("**Node-Specific Tips:**")
            md.append("")
            for notice in self.ui_elements['notices']:
                notice_text = notice.get('text', '')
                notice_name = notice.get('name', '')

                display_opts = notice.get('conditions', {})
                if display_opts:
                    show = display_opts.get('show', {})
                    conditions = []
                    for key, values in show.items():
                        if isinstance(values, list):
                            conditions.append(f"{key}={values}")
                        else:
                            conditions.append(f"{key}={values}")
                    cond_str = " when " + ", ".join(conditions) if conditions else ""
                else:
                    cond_str = ""

                md.append(f"- **{notice_name}**{cond_str}: {notice_text}")
            md.append("")

        # Credentials section
        if self.credentials:
            md.append("---")
            md.append("")
            md.append("## Required Credentials")
            md.append("")
            md.append("| Credential Type | Required | Conditions |")
            md.append("| --------------- | -------- | ---------- |")
            for cred in self.credentials:
                required = "" if cred.get('required') else "Optional"
                conditions = cred.get('displayOptions', '-')
                md.append(f"| `{cred['name']}` | {required} | {conditions} |")
            md.append("")

        # API Patterns section (if available)
        if self.api_patterns and any(self.api_patterns.values()):
            md.append("---")
            md.append("")
            md.append("## API Patterns")
            md.append("")
            if self.api_patterns.get('http_methods'):
                md.append(f"**HTTP Methods:** {', '.join(self.api_patterns['http_methods'])}")
                md.append("")
            if self.api_patterns.get('endpoints'):
                md.append("**Common Endpoints:**")
                for endpoint in self.api_patterns['endpoints'][:5]:  # Show top 5
                    md.append(f"- `{endpoint}`")
                md.append("")
            if self.api_patterns.get('headers'):
                md.append(f"**Headers Used:** {', '.join(list(self.api_patterns['headers'])[:10])}")
                md.append("")

        # Operations
        if self.operations or self.operations_by_resource:
            md.append("---")
            md.append("")
            md.append("## Operations")
            md.append("")

            if self.operations_by_resource:
                for resource_value, operations in self.operations_by_resource.items():
                    md.append(f"### {resource_value.title()} Resource Operations")
                    md.append("")
                    md.append("| Operation | ID | Description |")
                    md.append("| --------- | -- | ----------- |")
                    for op in operations:
                        op_name = op.get('name', '')
                        op_value = op.get('value', '')
                        op_desc = op.get('description', '') or op.get('action', '')
                        md.append(f"| {op_name} | `{op_value}` | {op_desc} |")
                    md.append("")
            elif self.operations:
                md.append("| Operation | ID | Description |")
                md.append("| --------- | -- | ----------- |")
                for op in self.operations:
                    op_name = op.get('name', '')
                    op_value = op.get('value', '')
                    op_desc = op.get('description', '')
                    md.append(f"| {op_name} | `{op_value}` | {op_desc} |")
                md.append("")

        # Parameters
        md.append("---")
        md.append("")
        md.append("## Parameters")
        md.append("")

        # Resource selector
        resource_param = next((p for p in self.properties if p.get('name') == 'resource'), None)
        if resource_param:
            md.append("### Resource Selector")
            md.append("")
            md.append("| Name | Field ID | Type | Default | Required | Description | Validation |")
            md.append("| ---- | -------- | ---- | ------- | :------: | ----------- | ---------- |")

            name = resource_param.get('displayName', 'Resource')
            field_id = resource_param.get('name', 'resource')
            ptype = resource_param.get('type', 'options')
            default = str(resource_param.get('default', ''))
            required = '' if resource_param.get('required') else ''
            desc = resource_param.get('description', 'Resource to operate on')

            md.append(f"| {name} | `{field_id}` | {ptype} | {default} | {required} | {desc} |  |")

            if resource_param.get('options'):
                md.append("")
                md.append(f"**{name} options:**")
                md.append("")
                for opt in resource_param.get('options', []):
                    opt_name = opt.get('name', '')
                    opt_value = opt.get('value', '')
                    opt_desc = opt.get('description', '')
                    opt_action = opt.get('action', '')

                    if opt_desc:
                        md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_desc}")
                    elif opt_action:
                        md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_action}")
                    else:
                        md.append(f"* **{opt_name}** (`{opt_value}`)")
                md.append("")

            md.append("---")
            md.append("")

        if self.operations:
            # Operation selector
            operation_param = next((p for p in self.properties if p.get('name') == 'operation'), None)
            if operation_param:
                md.append("### Operation Selector")
                md.append("")
                md.append("| Name | Field ID | Type | Default | Required | Description | Validation |")
                md.append("| ---- | -------- | ---- | ------- | :------: | ----------- | ---------- |")

                name = operation_param.get('displayName', 'Operation')
                field_id = operation_param.get('name', 'operation')
                ptype = operation_param.get('type', 'options')
                default = str(operation_param.get('default', ''))
                required = '' if operation_param.get('required') else ''
                desc = operation_param.get('description', 'Operation to perform')

                md.append(f"| {name} | `{field_id}` | {ptype} | {default} | {required} | {desc} |  |")

                if operation_param.get('options'):
                    md.append("")
                    md.append(f"**{name} options:**")
                    md.append("")
                    for opt in operation_param.get('options', []):
                        opt_name = opt.get('name', '')
                        opt_value = opt.get('value', '')
                        opt_desc = opt.get('description', '')
                        opt_action = opt.get('action', '')

                        if opt_desc:
                            md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_desc}")
                        elif opt_action:
                            md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_action}")
                        else:
                            md.append(f"* **{opt_name}** (`{opt_value}`)")
                    md.append("")

                md.append("---")
                md.append("")

            for op in self.operations:
                op_value = op.get('value', '')
                op_name = op.get('name', '')

                # Get parameters for this specific operation
                op_params = []
                for p in self.properties:
                    if p.get('name') in ['operation', 'resource']:
                        continue

                    display_opts = p.get('displayOptions', {}).get('show', {})
                    op_list = display_opts.get('operation', [])

                    if (p.get('_source') == op_value or
                        p.get('_operation') == op_value or
                        op_value in op_list):
                        op_params.append(p)

                if op_params:
                    md.append(f"### {op_name} parameters (`{op_value}`)")
                    md.append("")
                    md.append("| Name | Field ID | Type | Default | Required | Description | Validation |")
                    md.append("| ---- | -------- | ---- | ------- | :------: | ----------- | ---------- |")

                    for param in op_params:
                        if param.get('type') == 'notice':
                            continue

                        name = param.get('displayName', param.get('name', ''))
                        field_id = param.get('name', '')
                        ptype = param.get('type', '')
                        default = str(param.get('default', ''))
                        required = '' if param.get('required') else ''
                        desc = param.get('description', '')

                        # Build comprehensive description
                        desc_parts = []
                        if desc:
                            desc_parts.append(desc)

                        tooltip = param.get('tooltip', '')
                        if tooltip and tooltip != desc:
                            desc_parts.append(f"**Tooltip:** {tooltip}")

                        placeholder = param.get('placeholder', '')
                        hint = param.get('hint', '')
                        example = placeholder or hint
                        if example:
                            desc_parts.append(f"e.g. {example}")

                        desc = ' | '.join(desc_parts)

                        # Add validation info
                        validation_parts = []
                        if field_id in self.validation_rules:
                            rules = self.validation_rules[field_id]
                            if rules.get('format'):
                                validation_parts.append(rules['format'])
                            if rules.get('min') or rules.get('max'):
                                validation_parts.append(f"range:{rules.get('min', '?')}-{rules.get('max', '?')}")

                        type_opts = param.get('typeOptions', {})
                        if type_opts.get('minValue') is not None or type_opts.get('maxValue') is not None:
                            min_val = type_opts.get('minValue', '')
                            max_val = type_opts.get('maxValue', '')
                            validation_parts.append(f"min:{min_val}, max:{max_val}")

                        validation = ', '.join(validation_parts)

                        md.append(f"| {name} | `{field_id}` | {ptype} | {default} | {required} | {desc} | {validation} |")

                        # Show options for dropdown fields
                        if ptype in ['options', 'multiOptions'] and param.get('options'):
                            md.append("")
                            md.append(f"**{name} options:**")
                            md.append("")
                            for opt in param.get('options', []):
                                opt_name = opt.get('name', '')
                                opt_value = opt.get('value', '')
                                opt_desc = opt.get('description', '')
                                opt_action = opt.get('action', '')

                                if opt_desc:
                                    md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_desc}")
                                elif opt_action:
                                    md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_action}")
                                else:
                                    md.append(f"* **{opt_name}** (`{opt_value}`)")
                            md.append("")

                        # Show collection sub-options
                        if ptype in ['collection', 'fixedCollection'] and param.get('options'):
                            md.append("")
                            md.append(f"<details>")
                            md.append(f"<summary><strong>{name} sub-options</strong></summary>")
                            md.append("")
                            md.append("| Sub-Option | Field ID | Type | Default | Description |")
                            md.append("| ---------- | -------- | ---- | ------- | ----------- |")

                            for opt in param.get('options', []):
                                opt_name = opt.get('displayName', opt.get('name', ''))
                                opt_field = opt.get('name', '')
                                opt_type = opt.get('type', '')
                                opt_default = str(opt.get('default', ''))
                                opt_desc = opt.get('description', '')
                                md.append(f"| {opt_name} | `{opt_field}` | {opt_type} | {opt_default} | {opt_desc} |")

                            md.append("")
                            md.append("</details>")
                            md.append("")

                    md.append("")
        else:
            # No operations, list all parameters
            md.append("### Parameters")
            md.append("")
            md.append("| Name | Field ID | Type | Default | Required | Description | Validation |")
            md.append("| ---- | -------- | ---- | ------- | :------: | ----------- | ---------- |")

            for param in self.properties:
                if param.get('type') == 'notice':
                    continue

                name = param.get('displayName', param.get('name', ''))
                field_id = param.get('name', '')
                ptype = param.get('type', '')
                default = str(param.get('default', ''))
                required = '' if param.get('required') else ''
                desc = param.get('description', '')

                desc_parts = []
                if desc:
                    desc_parts.append(desc)

                tooltip = param.get('tooltip', '')
                if tooltip and tooltip != desc:
                    desc_parts.append(f"**Tooltip:** {tooltip}")

                placeholder = param.get('placeholder', '')
                hint = param.get('hint', '')
                example = placeholder or hint
                if example:
                    desc_parts.append(f"e.g. {example}")

                desc = ' | '.join(desc_parts)

                validation_parts = []
                if field_id in self.validation_rules:
                    rules = self.validation_rules[field_id]
                    if rules.get('format'):
                        validation_parts.append(rules['format'])
                    if rules.get('min') or rules.get('max'):
                        validation_parts.append(f"range:{rules.get('min', '?')}-{rules.get('max', '?')}")

                type_opts = param.get('typeOptions', {})
                if type_opts.get('minValue') is not None or type_opts.get('maxValue') is not None:
                    min_val = type_opts.get('minValue', '')
                    max_val = type_opts.get('maxValue', '')
                    validation_parts.append(f"min:{min_val}, max:{max_val}")

                validation = ', '.join(validation_parts)

                md.append(f"| {name} | `{field_id}` | {ptype} | {default} | {required} | {desc} | {validation} |")

                if ptype in ['options', 'multiOptions'] and param.get('options'):
                    md.append("")
                    md.append(f"**{name} options:**")
                    md.append("")
                    for opt in param.get('options', []):
                        opt_name = opt.get('name', '')
                        opt_value = opt.get('value', '')
                        opt_desc = opt.get('description', '')
                        opt_action = opt.get('action', '')

                        if opt_desc:
                            md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_desc}")
                        elif opt_action:
                            md.append(f"* **{opt_name}** (`{opt_value}`) - {opt_action}")
                        else:
                            md.append(f"* **{opt_name}** (`{opt_value}`)")
                    md.append("")

                if ptype in ['collection', 'fixedCollection'] and param.get('options'):
                    md.append("")
                    md.append(f"<details>")
                    md.append(f"<summary><strong>{name} sub-options</strong></summary>")
                    md.append("")
                    md.append("| Sub-Option | Field ID | Type | Default | Description |")
                    md.append("| ---------- | -------- | ---- | ------- | ----------- |")

                    for opt in param.get('options', []):
                        opt_name = opt.get('displayName', opt.get('name', ''))
                        opt_field = opt.get('name', '')
                        opt_type = opt.get('type', '')
                        opt_default = str(opt.get('default', ''))
                        opt_desc = opt.get('description', '')
                        md.append(f"| {opt_name} | `{opt_field}` | {opt_type} | {opt_default} | {opt_desc} |")

                    md.append("")
                    md.append("</details>")
                    md.append("")

            md.append("")

        # Load Options Methods
        if self.node_info.get('loadOptionsMethods'):
            md.append("---")
            md.append("")
            md.append("## Load Options Methods")
            md.append("")
            for method in self.node_info['loadOptionsMethods']:
                md.append(f"- `{method}`")
            md.append("")

        # Real-world examples
        if self.workflow_examples:
            md.append("---")
            md.append("")
            md.append("## Real-World Examples")
            md.append("")
            md.append("These examples are extracted from actual n8n workflows:")
            md.append("")

            for i, example in enumerate(self.workflow_examples[:5], 1):
                md.append(f"### Example {i}: {example['name']}")
                md.append("")
                md.append(f"**From workflow:** {example['workflow_name']}")
                md.append("")
                md.append("**Parameters:**")
                md.append("```json")
                md.append(json.dumps(example['parameters'], indent=2))
                md.append("```")
                md.append("")

                if example.get('credentials'):
                    md.append("**Credentials:**")
                    for cred_type, cred_info in example['credentials'].items():
                        md.append(f"- {cred_type}: `{cred_info.get('name', 'N/A')}`")
                    md.append("")

            md.append("")

        # Expression patterns
        if self.expressions:
            md.append("---")
            md.append("")
            md.append("## Common Expression Patterns")
            md.append("")
            md.append("These expression patterns are commonly used with this node:")
            md.append("")

            unique_expressions = list(set(self.expressions))[:10]
            for expr in unique_expressions:
                md.append(f"- `{expr}`")

            md.append("")

        # Execution Settings
        md.append("---")
        md.append("")

        # Determine which settings to show based on node type
        is_trigger = self.node_info.get('node_type') in ['trigger', 'webhook']

        if is_trigger:
            md.append("## Execution Settings")
            md.append("")
            md.append("**Note:** Trigger nodes have limited execution settings.")
        else:
            md.append("## Execution Settings")

        md.append("")
        md.append("| Name | Field ID | Type | Default | Description |")
        md.append("| ---- | -------- | ---- | ------- | ----------- |")

        # Add common settings (for all nodes)
        for setting_key, setting in COMMON_SETTINGS.items():
            name = setting['name']
            field_id = setting['field_id']
            stype = setting['type']
            default = str(setting.get('default', ''))
            desc = setting['description']
            md.append(f"| {name} | `{field_id}` | {stype} | {default} | {desc} |")

        # Add executable settings (only for non-trigger nodes)
        if not is_trigger:
            for setting_key, setting in EXECUTABLE_SETTINGS.items():
                name = setting['name']
                field_id = setting['field_id']
                stype = setting['type']
                default = str(setting.get('default', ''))
                desc = setting['description']

                # Add conditional note for maxTries and waitBetweenTries
                if setting.get('displayOptions'):
                    desc = f"{desc} *(Only visible when 'Retry On Fail' is enabled)*"

                # Add min/max constraints if present
                constraints = []
                if 'min' in setting:
                    constraints.append(f"min: {setting['min']}")
                if 'max' in setting:
                    constraints.append(f"max: {setting['max']}")
                if constraints:
                    desc = f"{desc} ({', '.join(constraints)})"

                md.append(f"| {name} | `{field_id}` | {stype} | {default} | {desc} |")

        md.append("")

        # Add On Error options details (only for executable nodes)
        if not is_trigger:
            md.append("**On Error Options:**")
            md.append("")
            for opt in EXECUTABLE_SETTINGS['onError']['options']:
                md.append(f"* **{opt['name']}** (`{opt['value']}`)  {opt['description']}")
            md.append("")

        # Notes & Caveats
        md.append("---")
        md.append("")
        md.append("## Notes & Caveats")
        md.append("")
        md.append(f"* This node is part of n8n-nodes-base")
        if self.metadata:
            if self.metadata.get('categories'):
                md.append(f"* Categories: {', '.join(self.metadata['categories'])}")
            if self.metadata.get('alias'):
                md.append(f"* Aliases: {', '.join(self.metadata['alias'])}")
        md.append("")

        # Machine-Readable Spec
        md.append("---")
        md.append("")
        md.append("## Appendix: Machine-Readable Spec (LLM-Friendly)")
        md.append("")

        # YAML Schema
        md.append("### YAML Schema")
        md.append("")
        md.append("```yaml")
        md.append(yaml.dump(self._generate_yaml_schema(), default_flow_style=False, sort_keys=False, allow_unicode=True, Dumper=yaml.Dumper))
        md.append("```")
        md.append("")

        # JSON Schema
        md.append("### JSON Schema")
        md.append("")
        md.append("```json")
        md.append(json.dumps(self._generate_json_schema(), indent=2))
        md.append("```")
        md.append("")

        # Document Changelog
        md.append("---")
        md.append("")
        md.append("## Document Changelog")
        md.append("")
        md.append("| Version | Date | Changes |")
        md.append("| ------- | ---- | ------- |")
        md.append(f"| {self.node_info.get('version', '1.0')} | {today} | Ultimate extraction with maximum detail for AI training |")
        md.append("")

        return '\n'.join(md)

    def _generate_yaml_schema(self) -> Dict[str, Any]:
        """Generate YAML schema with maximum detail"""
        schema = {
            'node': self.node_info['name'],
            'displayName': self.node_info['displayName'],
            'description': self.node_info['description'],
            'version': self.node_info.get('version', '1.0'),
            'nodeType': self.node_info.get('node_type', 'regular')
        }

        if self.node_info.get('group'):
            schema['group'] = self.node_info['group']

        # Add credentials
        if self.credentials:
            schema['credentials'] = [
                {
                    'name': c['name'],
                    'required': c.get('required', False)
                }
                for c in self.credentials
            ]

        # Add operations
        if self.operations:
            schema['operations'] = []
            for op in self.operations:
                op_schema = {
                    'id': op.get('value', ''),
                    'name': op.get('name', ''),
                    'description': op.get('description', '')
                }

                op_value = op.get('value', '')
                op_params = []
                for p in self.properties:
                    if p.get('name') in ['operation', 'resource']:
                        continue

                    display_opts = p.get('displayOptions', {}).get('show', {})
                    op_list = display_opts.get('operation', [])

                    if (p.get('_source') == op_value or
                        p.get('_operation') == op_value or
                        op_value in op_list):
                        op_params.append(p)

                if op_params:
                    op_schema['params'] = []
                    for param in op_params:
                        if param.get('type') not in ['notice', 'collection']:
                            param_schema = {
                                'id': param.get('name', ''),
                                'name': param.get('displayName', ''),
                                'type': param.get('type', ''),
                                'default': param.get('default', ''),
                                'required': param.get('required', False),
                                'description': param.get('description', '')
                            }
                            if param.get('hint'):
                                param_schema['hint'] = param['hint']
                            if param.get('placeholder'):
                                param_schema['placeholder'] = param['placeholder']
                            if param.get('tooltip'):
                                param_schema['tooltip'] = param['tooltip']

                            # Add validation info
                            if param.get('name') in self.validation_rules:
                                param_schema['validation'] = self.validation_rules[param['name']]

                            # Add type info
                            if param.get('name') in self.type_info:
                                param_schema['typeInfo'] = self.type_info[param['name']]

                            op_schema['params'].append(param_schema)

                schema['operations'].append(op_schema)
        else:
            schema['params'] = []
            for param in self.properties:
                if param.get('type') != 'notice':
                    param_schema = {
                        'id': param.get('name', ''),
                        'name': param.get('displayName', ''),
                        'type': param.get('type', ''),
                        'default': param.get('default', ''),
                        'required': param.get('required', False),
                        'description': param.get('description', '')
                    }
                    if param.get('hint'):
                        param_schema['hint'] = param['hint']
                    if param.get('placeholder'):
                        param_schema['placeholder'] = param['placeholder']
                    if param.get('tooltip'):
                        param_schema['tooltip'] = param['tooltip']

                    if param.get('name') in self.validation_rules:
                        param_schema['validation'] = self.validation_rules[param['name']]

                    if param.get('name') in self.type_info:
                        param_schema['typeInfo'] = self.type_info[param['name']]

                    schema['params'].append(param_schema)

        # Add examples
        if self.workflow_examples:
            schema['examples'] = [
                {
                    'name': ex['name'],
                    'parameters': ex['parameters'],
                    'workflow': ex['workflow_name']
                }
                for ex in self.workflow_examples[:3]
            ]

        # Add expression patterns
        if self.expressions:
            schema['common_expressions'] = list(set(self.expressions))[:10]

        # Add API patterns
        if self.api_patterns:
            schema['api_patterns'] = self.api_patterns

        # Add UI elements
        schema['ui_elements'] = self.ui_elements

        # Add execution settings (based on node type)
        is_trigger = self.node_info.get('node_type') in ['trigger', 'webhook']
        if is_trigger:
            schema['settings'] = {
                'common': COMMON_SETTINGS,
                'note': 'Trigger nodes only have common settings (notes, notesInFlow)'
            }
        else:
            schema['settings'] = {
                'common': COMMON_SETTINGS,
                'executable': EXECUTABLE_SETTINGS,
                'note': 'Executable nodes have both common and executable settings'
            }

        return schema

    def _generate_json_schema(self) -> Dict[str, Any]:
        """Generate JSON schema with maximum detail"""
        schema = {
            "$id": f"https://n8n.io/schemas/nodes/{self.node_info['name']}.json",
            "$schema": "http://json-schema.org/draft-07/schema#",
            "title": f"{self.node_info['displayName']} Node",
            "type": "object",
            "additionalProperties": False,
            "properties": {},
            "metadata": {
                "nodeType": self.node_info.get('node_type', 'regular'),
                "version": self.node_info.get('version', '1.0')
            }
        }

        if self.operations:
            schema["properties"]["operation"] = {
                "type": "string",
                "enum": [op.get('value', '') for op in self.operations],
                "description": "Operation to perform"
            }

        schema["properties"]["params"] = {
            "type": "object",
            "additionalProperties": False,
            "properties": {}
        }

        # Add parameters with validation rules
        for param in self.properties:
            if param.get('type') == 'notice':
                continue

            param_name = param.get('name', '')
            param_type = param.get('type', '')

            param_schema = {
                "description": param.get('description', '')
            }

            if param_type == 'string':
                param_schema["type"] = "string"
            elif param_type == 'boolean':
                param_schema["type"] = "boolean"
            elif param_type == 'number':
                param_schema["type"] = "number"
            elif param_type == 'options':
                param_schema["type"] = "string"
                if param.get('options'):
                    param_schema["enum"] = [opt.get('value', opt.get('name', '')) for opt in param['options']]
            else:
                param_schema["type"] = "string"

            if param.get('default') is not None:
                param_schema["default"] = param['default']

            # Add validation constraints
            if param_name in self.validation_rules:
                rules = self.validation_rules[param_name]
                if rules.get('format'):
                    param_schema["format"] = rules['format']
                if rules.get('min'):
                    param_schema["minimum"] = rules['min']
                if rules.get('max'):
                    param_schema["maximum"] = rules['max']

            # Add UI hints
            if param.get('placeholder'):
                param_schema["examples"] = [param['placeholder']]
            if param.get('tooltip'):
                param_schema["tooltip"] = param['tooltip']

            schema["properties"]["params"]["properties"][param_name] = param_schema

        # Add credentials to schema
        if self.credentials:
            schema["credentials"] = [
                {
                    "name": c['name'],
                    "required": c.get('required', False)
                }
                for c in self.credentials
            ]

        # Add examples
        if self.workflow_examples:
            schema["examples"] = [
                {
                    "description": ex['name'],
                    "value": ex['parameters']
                }
                for ex in self.workflow_examples[:3]
            ]

        # Add execution settings schema
        is_trigger = self.node_info.get('node_type') in ['trigger', 'webhook']
        settings_properties = {}

        # Add common settings
        for key, setting in COMMON_SETTINGS.items():
            settings_properties[key] = {
                "type": setting['type'],
                "default": setting.get('default'),
                "description": setting['description']
            }

        # Add executable settings (only for non-trigger nodes)
        if not is_trigger:
            for key, setting in EXECUTABLE_SETTINGS.items():
                prop_schema = {
                    "type": setting['type'],
                    "default": setting.get('default'),
                    "description": setting['description']
                }
                # Add min/max for number types
                if setting['type'] == 'number':
                    if 'min' in setting:
                        prop_schema["minimum"] = setting['min']
                    if 'max' in setting:
                        prop_schema["maximum"] = setting['max']
                # Add enum for options type
                if setting['type'] == 'options' and 'options' in setting:
                    prop_schema["enum"] = [opt['value'] for opt in setting['options']]

                settings_properties[key] = prop_schema

        schema["properties"]["settings"] = {
            "type": "object",
            "additionalProperties": False,
            "properties": settings_properties
        }

        return schema

    @staticmethod
    def _normalize_filename(name: str) -> str:
        """
        Normalize node name for consistent, safe filenames
        - Convert to lowercase
        - Remove/replace special characters
        - Replace spaces with underscores
        """
        # Convert to lowercase
        normalized = name.lower()
        # Replace spaces and hyphens with underscores
        normalized = normalized.replace(' ', '_').replace('-', '_')
        # Remove any characters that aren't alphanumeric or underscore
        normalized = re.sub(r'[^a-z0-9_]', '', normalized)
        # Remove duplicate underscores
        normalized = re.sub(r'_+', '_', normalized)
        # Remove leading/trailing underscores
        normalized = normalized.strip('_')
        return normalized

    def save_output(self, markdown: str):
        """Save generated documentation with enhanced data"""
        OUTPUT_DIR.mkdir(exist_ok=True)

        # Use normalized filename for consistency
        filename_base = self._normalize_filename(self.node_info['name'])

        # Save markdown
        md_file = OUTPUT_DIR / f"{filename_base}_documentation.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown)

        # Save comprehensive JSON data
        json_file = OUTPUT_DIR / f"{filename_base}_data.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'node_info': self.node_info,
                'metadata': self.metadata,
                'properties': self.properties,
                'operations': self.operations,
                'operations_by_resource': self.operations_by_resource,
                'credentials': self.credentials,
                'workflow_examples': self.workflow_examples,
                'expressions': list(set(self.expressions)),
                'validation_rules': self.validation_rules,
                'type_info': self.type_info,
                'api_patterns': self.api_patterns,
                'ui_elements': self.ui_elements,
                'settings': self._get_node_settings()
            }, f, indent=2)

        return md_file

    def _get_node_settings(self) -> Dict[str, Any]:
        """Get appropriate settings based on node type"""
        is_trigger = self.node_info.get('node_type') in ['trigger', 'webhook']
        if is_trigger:
            return {
                'common': COMMON_SETTINGS,
                'note': 'Trigger nodes only have common settings (notes, notesInFlow)'
            }
        else:
            return {
                'common': COMMON_SETTINGS,
                'executable': EXECUTABLE_SETTINGS,
                'note': 'Executable nodes have both common and executable settings'
            }

    def extract(self, node_file_path: Optional[Path] = None) -> bool:
        """Main extraction method with AI enhancements"""
        print(f"\n{'='*70}")
        print(f"Extracting: {self.node_name}")
        print('='*70 + "\n")

        if node_file_path:
            if not self.set_node_file(node_file_path):
                return False
        else:
            if not self.find_node():
                return False

        if not self.load_files():
            return False

        print()
        self.extract_info()

        # Print extraction summary
        print(f" Extracted {len(self.properties)} properties")
        print(f" Found {len(self.operations)} operations")
        if self.credentials:
            print(f" Found {len(self.credentials)} credential types")
        if self.workflow_examples:
            print(f" Found {len(self.workflow_examples)} workflow examples")
        if self.expressions:
            print(f" Found {len(set(self.expressions))} unique expressions")
        if self.api_patterns and self.api_patterns.get('endpoints'):
            print(f" Found {len(self.api_patterns['endpoints'])} API endpoints")
        if self.ui_elements['notices']:
            print(f" Found {len(self.ui_elements['notices'])} UI notices")

        markdown = self.generate_markdown()
        self.save_output(markdown)

        return True


def list_all_nodes():
    """List all available nodes in the repository"""
    nodes_dir = N8N_REPO / "packages" / "nodes-base" / "nodes"

    if not nodes_dir.exists():
        print(f" Error: Cannot find nodes directory at {nodes_dir}")
        return

    node_files = sorted(nodes_dir.glob("**/*.node.ts"))

    print(f"\n{'='*70}")
    print(f"Available Nodes ({len(node_files)} total)")
    print('='*70 + "\n")

    for node_file in node_files:
        node_name = node_file.stem
        rel_path = node_file.relative_to(nodes_dir)
        print(f"   {node_name:40s} ({rel_path.parent})")

    print()


def extract_all_nodes():
    """Extract all nodes with AI training enhancements"""
    nodes_dir = N8N_REPO / "packages" / "nodes-base" / "nodes"

    if not nodes_dir.exists():
        print(f" Error: Cannot find nodes directory at {nodes_dir}")
        return

    node_files = sorted(nodes_dir.glob("**/*.node.ts"))

    print(f"\n{'='*70}")
    print(f"Ultimate Extraction: All Nodes ({len(node_files)} total)")
    print('='*70 + "\n")

    successful = 0
    failed = 0

    for i, node_file in enumerate(node_files, 1):
        node_name = node_file.stem.replace('.node', '')

        print(f"\n[{i}/{len(node_files)}] {node_name}")

        try:
            extractor = NodeExtractor(node_name)
            if extractor.extract(node_file_path=node_file):
                successful += 1
            else:
                failed += 1
        except Exception as e:
            print(f" Error: {str(e)[:100]}")
            failed += 1

    print(f"\n{'='*70}")
    print(f"Ultimate Extraction Complete!")
    print('='*70)
    print(f"\n Successful: {successful}")
    print(f" Failed: {failed}")
    print(f"\nTotal: {len(node_files)}")
    print(f"\nDocumentation saved to: {OUTPUT_DIR}")
    print()


def main():
    if len(sys.argv) < 2:
        print("""
Usage: python3 n8n_node_extractor.py [command] [node_name]

Commands:
  extract <node_name>  - Extract a specific node with maximum detail
  list                 - List all available nodes
  extract-all          - Extract all nodes with full AI training features

Examples:
  python3 n8n_node_extractor.py extract ReadWriteFile
  python3 n8n_node_extractor.py list
  python3 n8n_node_extractor.py extract-all
        """)
        return

    command = sys.argv[1]

    if command == "list":
        list_all_nodes()
    elif command == "extract-all":
        extract_all_nodes()
    elif command == "extract":
        if len(sys.argv) < 3:
            print(" Error: Please provide a node name")
            print("\nUsage: python3 n8n_node_extractor.py extract <node_name>")
            return

        node_name = sys.argv[2]
        extractor = NodeExtractor(node_name)
        extractor.extract()
    else:
        print(f" Error: Unknown command '{command}'")
        print("\nUse 'list', 'extract', or 'extract-all'")


if __name__ == "__main__":
    main()
